{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from scipy.signal import firwin, filtfilt\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import warnings\n",
    "import torch\n",
    "import random\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"mne\")\n",
    "# Completely silence MNE-Python output\n",
    "mne.set_log_level('WARNING')  # or 'ERROR' for even less output\n",
    "logging.getLogger('mne').setLevel(logging.WARNING)\n",
    "#mne.set_log_level('debug')\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eeg(path):\n",
    "    raw = mne.io.read_raw_gdf(path, preload=True)\n",
    "\n",
    "    # Step 2: Get the sampling frequency\n",
    "    sfreq = raw.info['sfreq']  # Hz\n",
    "\n",
    "    # Step 3: Define FIR filter parameters\n",
    "    low_cutoff = 1.0   # Hz\n",
    "    high_cutoff = 30.0 # Hz\n",
    "    filter_order = 177 # Must be odd for linear-phase FIR\n",
    "\n",
    "    nyquist = 0.5 * sfreq\n",
    "\n",
    "\n",
    "    fir_coeffs = firwin(\n",
    "        numtaps=filter_order,\n",
    "        cutoff=[low_cutoff / nyquist, high_cutoff / nyquist],\n",
    "        pass_zero=False,\n",
    "        window='blackman'\n",
    "    )\n",
    "    eeg_data = raw.get_data()\n",
    "    filtered_data = filtfilt(fir_coeffs, 1.0, eeg_data, axis=1)\n",
    "\n",
    "    new_raw = mne.io.RawArray(filtered_data, raw.info.copy())\n",
    "    annotations = raw.annotations \n",
    "    new_raw.set_annotations(annotations)\n",
    "\n",
    "    return new_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path, test=False):\n",
    "    raw = load_eeg(path)\n",
    "    eeg_channels = raw.ch_names[:22]\n",
    "    raw.pick(eeg_channels)\n",
    "    \n",
    "    if not test:\n",
    "        events, events_id = mne.events_from_annotations(raw, event_id= {'769': 0,'770': 1,'771': 2,'772': 3})\n",
    "    else:\n",
    "        events, events_id = mne.events_from_annotations(raw, event_id= {'768':6})\n",
    "        #print(events_id)\n",
    "    #print(events_id)\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events=events,  \n",
    "        tmin=0,     \n",
    "        tmax=6.0,\n",
    "        event_id=events_id,\n",
    "        baseline=None,\n",
    "        preload=True\n",
    "    )\n",
    "\n",
    "    labels = epochs.events[:, 2]\n",
    "    #print(labels)\n",
    "    data = epochs.get_data()\n",
    "    \n",
    "    return {\n",
    "        'epochs': data,   \n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "#a = preprocess(\"E:/LOKI/BCI-IV/A01T.gdf\")\n",
    "# print(a['epochs'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = preprocess('/teamspace/studios/this_studio/EEG_REC/A01T.gdf')\n",
    "# print(a['epochs'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, num_channels, reduction_ratio=4):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_channels, num_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channels // reduction_ratio, num_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel_avg = torch.mean(x, dim=2, keepdim=True)  \n",
    "        channel_max, _ = torch.max(x, dim=2, keepdim=True) \n",
    "        combined = channel_avg + channel_max  \n",
    "        combined = combined.squeeze(2)\n",
    "        attention = self.mlp(combined)  # (batch_size, channels)\n",
    "        attention = torch.sigmoid(attention).unsqueeze(2) # (batch_size, 1, 1, channels)\n",
    "        attended_x = x * attention\n",
    "        return attended_x, attention.squeeze()\n",
    "\n",
    "class LearnableSTFT(nn.Module):\n",
    "\n",
    "    def __init__(self, window_size, hop_size, dft_size=None):\n",
    "        super(LearnableSTFT, self).__init__()\n",
    "\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.hop_size = hop_size\n",
    "        self.dft_size = dft_size if dft_size is not None else window_size\n",
    "\n",
    "        initial_window = 0.54 - 0.46 * torch.cos(\n",
    "            2 * math.pi * torch.arange(window_size, dtype=torch.float32) / (window_size - 1)\n",
    "        )\n",
    "\n",
    "        self.window = nn.Parameter(initial_window)\n",
    "\n",
    "        dft_matrix = self._create_dft_matrix(self.dft_size, self.window_size)\n",
    "        \n",
    "        self.register_buffer('dft_matrix', dft_matrix)\n",
    "\n",
    "    def _create_dft_matrix(self, dft_size, window_size):\n",
    "\n",
    "        k = torch.arange(dft_size).unsqueeze(1) # Shape: [dft_size, 1]\n",
    "        n = torch.arange(window_size)            # Shape: [window_size]\n",
    "\n",
    "        # Calculate the angle for the complex exponential\n",
    "        angle = -2 * math.pi * k * n / dft_size\n",
    "\n",
    "        # Use Euler's formula to create the complex matrix\n",
    "        # e^(j*angle) = cos(angle) + j*sin(angle)\n",
    "        dft_matrix = torch.complex(torch.cos(angle), torch.sin(angle))\n",
    "        return dft_matrix\n",
    "\n",
    "    def forward(self, signal):\n",
    "\n",
    "        if signal.dim() == 1:\n",
    "            signal = signal.unsqueeze(0).unsqueeze(0) # [1, 1, T]\n",
    "        elif signal.dim() == 2:\n",
    "            signal = signal.unsqueeze(1) # [B, 1, T]\n",
    "        batch_size, num_channels, num_samples = signal.shape\n",
    "\n",
    "        signal_reshaped = signal.reshape(batch_size * num_channels, num_samples)\n",
    "\n",
    "        learnable_window = self.window\n",
    "        frames = signal_reshaped.unfold(dimension=1, size=self.window_size, step=self.hop_size)\n",
    "\n",
    "        num_frames_unfolded = frames.shape[1]\n",
    "        expected_num_frames = int(math.ceil((num_samples - self.window_size) / self.hop_size)) + 1\n",
    "\n",
    "        if num_frames_unfolded < expected_num_frames:\n",
    "            padding_amount = (expected_num_frames - 1) * self.hop_size + self.window_size - num_samples\n",
    "            padded_signal = torch.nn.functional.pad(signal_reshaped, (0, padding_amount))\n",
    "            frames = padded_signal.unfold(1, self.window_size, self.hop_size)\n",
    "\n",
    "        windowed_frames = frames * learnable_window\n",
    "\n",
    "        BC, F, W = windowed_frames.shape # BC = batch_size * num_channels\n",
    "        windowed_frames_reshaped = windowed_frames.reshape(BC * F, W)\n",
    "\n",
    "        windowed_frames_complex = windowed_frames_reshaped.to(self.dft_matrix.dtype)\n",
    "        stft_result_reshaped = self.dft_matrix @ windowed_frames_complex.T\n",
    "        stft_result = stft_result_reshaped.T.reshape(batch_size, num_channels, F, self.dft_size)\n",
    "\n",
    "        return stft_result\n",
    "\n",
    "class Attention4D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, time_frames, freq_bins, d_model=128, n_heads=4, d_ff=256, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.time_frames = time_frames\n",
    "\n",
    "        in_features = in_channels * freq_bins\n",
    "\n",
    "        self.projection = nn.Linear(in_features, d_model)\n",
    "\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, time_frames, d_model))\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "        x = x.reshape(batch_size, self.time_frames, -1)\n",
    "\n",
    "        x = self.projection(x) # Output: (batch, time_frames, d_model)\n",
    "\n",
    "        x = x + self.positional_encoding\n",
    "\n",
    "        x_norm1 = self.layernorm1(x)\n",
    "        attn_output, _ = self.attention(x_norm1, x_norm1, x_norm1)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "\n",
    "        x_norm2 = self.layernorm2(x)\n",
    "        ff_output = self.feed_forward(x_norm2)\n",
    "        x = x + self.dropout2(ff_output)\n",
    "\n",
    "        return x\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1) # A flexible way to do global average pooling\n",
    "        self.classifier = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.permute(0, 2, 1)  # Shape -> (4, 128, 17)\n",
    "\n",
    "        x = self.pooling(x).squeeze(2) # Shape -> (4, 128)\n",
    "\n",
    "        output = self.classifier(x) # Shape -> (4, num_classes)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class EEGClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(EEGClassifier, self).__init__()\n",
    "        self.channel_attn = ChannelAttention(num_channels = 22)\n",
    "        self.learnable_stft = LearnableSTFT(window_size=250, hop_size=100)\n",
    "        \n",
    "        self.attention_module = Attention4D(\n",
    "            in_channels=22,\n",
    "            time_frames=14,\n",
    "            freq_bins=250,\n",
    "            d_model=128,\n",
    "            n_heads=8\n",
    "        )\n",
    "        self.classifier = Classifier(in_features=128, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x,_ = self.channel_attn(x)\n",
    "        x = self.learnable_stft(x)\n",
    "        x = torch.abs(x)\n",
    "        x = self.attention_module(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#start of trials (768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BCI4_2a_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, subjects=[1], train=True, transform=None, target_transform=None, test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.subjects = subjects\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.test=test\n",
    "        self.data, self.labels = self._load_data()\n",
    "        \n",
    "        self.labels = self.labels \n",
    "        \n",
    "    def _load_data(self):\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for subject in self.subjects:\n",
    "            if self.train:\n",
    "                filename = f'A{subject:02d}T.gdf'\n",
    "            else:\n",
    "                filename = f'A{subject:02d}E.gdf'\n",
    "                \n",
    "            filepath = os.path.join(self.data_dir, filename)\n",
    "            data_ = preprocess(filepath, self.test)\n",
    "            \n",
    "            data = data_['epochs']\n",
    "            labels = data_['labels']\n",
    "            #data = np.transpose(data, (0, 2, 1))\n",
    "            \n",
    "            all_data.append(data)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "        all_data = np.concatenate(all_data, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        #print(all_labels.shape)\n",
    "        def normalize_eeg(trial_data):\n",
    "            \"\"\"trial_data shape: (channels, timepoints)\"\"\"\n",
    "            means = trial_data.mean(axis=1, keepdims=True)\n",
    "            stds = trial_data.std(axis=1, keepdims=True)\n",
    "            return (trial_data - means) / (stds + 1e-8)\n",
    "\n",
    "        all_data_n = normalize_eeg(all_data)\n",
    "\n",
    "        return all_data_n, all_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        x = torch.from_numpy(x).float()  # shape: (1, timepoints, channels)\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "def get_data_loaders(data_dir, subjects=[1], batch_size=32, val_split=0.2, test_split=0.2, random_state=42):\n",
    "\n",
    "    full_train_dataset = BCI4_2a_Dataset(data_dir, subjects=subjects, train=True)\n",
    "    \n",
    "    test_dataset = BCI4_2a_Dataset(data_dir, subjects=subjects, train=False, test=True )\n",
    "    \n",
    "    train_idx, val_idx = train_test_split(\n",
    "        range(len(full_train_dataset)),\n",
    "        test_size=val_split,\n",
    "        random_state=random_state,\n",
    "        stratify=full_train_dataset.labels\n",
    "    )\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(full_train_dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(full_train_dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     DATA_DIR = \"/teamspace/studios/this_studio/EEG_REC\"\n",
    "    \n",
    "#     train_loader, val_loader, test_loader = get_data_loaders(\n",
    "#         data_dir=DATA_DIR,\n",
    "#         subjects=[1,2,3,5,6,7,8,9],\n",
    "#         batch_size=32\n",
    "#     )\n",
    "    \n",
    "#     print(\"Dataset sizes:\")\n",
    "#     print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "#     print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "#     print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "    \n",
    "#     x, y = next(iter(train_loader))\n",
    "#     print(\"\\nBatch shape:\")\n",
    "#     print(f\"Input shape: {x.shape}\")  # Should be (batch_size, 1, timepoints, channels)\n",
    "#     print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 17:55:46.087202: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-02 17:55:46.501841: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-02 17:55:46.501947: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-02 17:55:46.504334: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-02 17:55:46.766687: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-02 17:55:48.571370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Epoch: 1 | Loss: 3.1257 | Acc: 26.58%\n",
      "Val - Epoch: 1 | Loss: 1.4869 | Acc: 31.74%\n",
      "Train - Epoch: 2 | Loss: 1.4734 | Acc: 31.54%\n",
      "Val - Epoch: 2 | Loss: 1.5332 | Acc: 29.78%\n",
      "Train - Epoch: 3 | Loss: 1.4785 | Acc: 32.35%\n",
      "Val - Epoch: 3 | Loss: 1.5823 | Acc: 26.30%\n",
      "Train - Epoch: 4 | Loss: 1.4291 | Acc: 34.69%\n",
      "Val - Epoch: 4 | Loss: 1.3433 | Acc: 35.00%\n",
      "Train - Epoch: 5 | Loss: 1.2677 | Acc: 41.72%\n",
      "Val - Epoch: 5 | Loss: 1.2090 | Acc: 45.87%\n",
      "Train - Epoch: 6 | Loss: 1.2233 | Acc: 45.70%\n",
      "Val - Epoch: 6 | Loss: 1.1974 | Acc: 47.17%\n",
      "Train - Epoch: 7 | Loss: 1.0587 | Acc: 56.37%\n",
      "Val - Epoch: 7 | Loss: 1.2052 | Acc: 48.91%\n",
      "Train - Epoch: 8 | Loss: 0.9946 | Acc: 59.10%\n",
      "Val - Epoch: 8 | Loss: 1.4230 | Acc: 39.78%\n",
      "Train - Epoch: 9 | Loss: 0.8129 | Acc: 69.99%\n",
      "Val - Epoch: 9 | Loss: 1.1989 | Acc: 49.78%\n",
      "Train - Epoch: 10 | Loss: 0.6990 | Acc: 74.46%\n",
      "Val - Epoch: 10 | Loss: 1.2951 | Acc: 46.74%\n",
      "Train - Epoch: 11 | Loss: 0.5685 | Acc: 79.30%\n",
      "Val - Epoch: 11 | Loss: 1.2700 | Acc: 48.04%\n",
      "Train - Epoch: 12 | Loss: 0.4123 | Acc: 87.80%\n",
      "Val - Epoch: 12 | Loss: 1.3738 | Acc: 47.39%\n",
      "Train - Epoch: 13 | Loss: 0.2909 | Acc: 92.48%\n",
      "Val - Epoch: 13 | Loss: 1.5525 | Acc: 46.96%\n",
      "Train - Epoch: 14 | Loss: 0.1818 | Acc: 96.62%\n",
      "Val - Epoch: 14 | Loss: 1.5770 | Acc: 48.26%\n",
      "Train - Epoch: 15 | Loss: 0.0924 | Acc: 99.18%\n",
      "Val - Epoch: 15 | Loss: 1.7539 | Acc: 47.17%\n",
      "Train - Epoch: 16 | Loss: 0.0472 | Acc: 99.89%\n",
      "Val - Epoch: 16 | Loss: 1.9735 | Acc: 46.09%\n",
      "Train - Epoch: 17 | Loss: 0.0305 | Acc: 100.00%\n",
      "Val - Epoch: 17 | Loss: 2.0158 | Acc: 48.04%\n",
      "Train - Epoch: 18 | Loss: 0.0168 | Acc: 100.00%\n",
      "Val - Epoch: 18 | Loss: 2.0568 | Acc: 48.04%\n",
      "Train - Epoch: 19 | Loss: 0.0124 | Acc: 100.00%\n",
      "Val - Epoch: 19 | Loss: 2.1061 | Acc: 47.39%\n",
      "Train - Epoch: 20 | Loss: 0.0104 | Acc: 100.00%\n",
      "Val - Epoch: 20 | Loss: 2.1494 | Acc: 47.83%\n",
      "Train - Epoch: 21 | Loss: 0.0085 | Acc: 100.00%\n",
      "Val - Epoch: 21 | Loss: 2.1910 | Acc: 46.74%\n",
      "Train - Epoch: 22 | Loss: 0.0075 | Acc: 100.00%\n",
      "Val - Epoch: 22 | Loss: 2.2597 | Acc: 46.30%\n",
      "Train - Epoch: 23 | Loss: 0.0066 | Acc: 100.00%\n",
      "Val - Epoch: 23 | Loss: 2.2901 | Acc: 46.52%\n",
      "Train - Epoch: 24 | Loss: 0.0056 | Acc: 100.00%\n",
      "Val - Epoch: 24 | Loss: 2.3142 | Acc: 47.39%\n",
      "Train - Epoch: 25 | Loss: 0.0049 | Acc: 100.00%\n",
      "Val - Epoch: 25 | Loss: 2.3606 | Acc: 46.74%\n",
      "Train - Epoch: 26 | Loss: 0.0043 | Acc: 100.00%\n",
      "Val - Epoch: 26 | Loss: 2.3626 | Acc: 47.17%\n",
      "Train - Epoch: 27 | Loss: 0.0042 | Acc: 100.00%\n",
      "Val - Epoch: 27 | Loss: 2.3813 | Acc: 47.17%\n",
      "Train - Epoch: 28 | Loss: 0.0039 | Acc: 100.00%\n",
      "Val - Epoch: 28 | Loss: 2.4026 | Acc: 46.52%\n",
      "Train - Epoch: 29 | Loss: 0.0036 | Acc: 100.00%\n",
      "Val - Epoch: 29 | Loss: 2.4134 | Acc: 48.04%\n",
      "Train - Epoch: 30 | Loss: 0.0034 | Acc: 100.00%\n",
      "Val - Epoch: 30 | Loss: 2.4318 | Acc: 47.17%\n",
      "Train - Epoch: 31 | Loss: 0.0033 | Acc: 100.00%\n",
      "Val - Epoch: 31 | Loss: 2.4494 | Acc: 47.61%\n",
      "Train - Epoch: 32 | Loss: 0.0032 | Acc: 100.00%\n",
      "Val - Epoch: 32 | Loss: 2.4690 | Acc: 47.61%\n",
      "Train - Epoch: 33 | Loss: 0.0030 | Acc: 100.00%\n",
      "Val - Epoch: 33 | Loss: 2.4911 | Acc: 46.52%\n",
      "Train - Epoch: 34 | Loss: 0.0027 | Acc: 100.00%\n",
      "Val - Epoch: 34 | Loss: 2.4916 | Acc: 47.61%\n",
      "Train - Epoch: 35 | Loss: 0.0027 | Acc: 100.00%\n",
      "Val - Epoch: 35 | Loss: 2.5001 | Acc: 46.74%\n",
      "Train - Epoch: 36 | Loss: 0.0026 | Acc: 100.00%\n",
      "Val - Epoch: 36 | Loss: 2.5125 | Acc: 47.17%\n",
      "Train - Epoch: 37 | Loss: 0.0025 | Acc: 100.00%\n",
      "Val - Epoch: 37 | Loss: 2.5209 | Acc: 47.61%\n",
      "Train - Epoch: 38 | Loss: 0.0023 | Acc: 100.00%\n",
      "Val - Epoch: 38 | Loss: 2.5262 | Acc: 47.83%\n",
      "Train - Epoch: 39 | Loss: 0.0024 | Acc: 100.00%\n",
      "Val - Epoch: 39 | Loss: 2.5371 | Acc: 46.96%\n",
      "Train - Epoch: 40 | Loss: 0.0022 | Acc: 100.00%\n",
      "Val - Epoch: 40 | Loss: 2.5441 | Acc: 48.04%\n",
      "Train - Epoch: 41 | Loss: 0.0022 | Acc: 100.00%\n",
      "Val - Epoch: 41 | Loss: 2.5528 | Acc: 47.17%\n",
      "Train - Epoch: 42 | Loss: 0.0022 | Acc: 100.00%\n",
      "Val - Epoch: 42 | Loss: 2.5610 | Acc: 46.74%\n",
      "Train - Epoch: 43 | Loss: 0.0021 | Acc: 100.00%\n",
      "Val - Epoch: 43 | Loss: 2.5637 | Acc: 48.04%\n",
      "Train - Epoch: 44 | Loss: 0.0020 | Acc: 100.00%\n",
      "Val - Epoch: 44 | Loss: 2.5674 | Acc: 47.61%\n",
      "Train - Epoch: 45 | Loss: 0.0020 | Acc: 100.00%\n",
      "Val - Epoch: 45 | Loss: 2.5728 | Acc: 47.61%\n",
      "Train - Epoch: 46 | Loss: 0.0020 | Acc: 100.00%\n",
      "Val - Epoch: 46 | Loss: 2.5793 | Acc: 46.96%\n",
      "Train - Epoch: 47 | Loss: 0.0019 | Acc: 100.00%\n",
      "Val - Epoch: 47 | Loss: 2.5836 | Acc: 47.61%\n",
      "Train - Epoch: 48 | Loss: 0.0019 | Acc: 100.00%\n",
      "Val - Epoch: 48 | Loss: 2.5881 | Acc: 48.04%\n",
      "Train - Epoch: 49 | Loss: 0.0019 | Acc: 100.00%\n",
      "Val - Epoch: 49 | Loss: 2.5975 | Acc: 46.52%\n",
      "Train - Epoch: 50 | Loss: 0.0019 | Acc: 100.00%\n",
      "Val - Epoch: 50 | Loss: 2.5992 | Acc: 47.17%\n",
      "Train - Epoch: 51 | Loss: 0.0018 | Acc: 100.00%\n",
      "Val - Epoch: 51 | Loss: 2.6013 | Acc: 47.17%\n",
      "Train - Epoch: 52 | Loss: 0.0018 | Acc: 100.00%\n",
      "Val - Epoch: 52 | Loss: 2.6048 | Acc: 47.39%\n",
      "Train - Epoch: 53 | Loss: 0.0018 | Acc: 100.00%\n",
      "Val - Epoch: 53 | Loss: 2.6071 | Acc: 47.61%\n",
      "Train - Epoch: 54 | Loss: 0.0018 | Acc: 100.00%\n",
      "Val - Epoch: 54 | Loss: 2.6111 | Acc: 47.17%\n",
      "Train - Epoch: 55 | Loss: 0.0018 | Acc: 100.00%\n",
      "Val - Epoch: 55 | Loss: 2.6150 | Acc: 46.96%\n",
      "Train - Epoch: 56 | Loss: 0.0017 | Acc: 100.00%\n",
      "Val - Epoch: 56 | Loss: 2.6164 | Acc: 47.17%\n",
      "Train - Epoch: 57 | Loss: 0.0017 | Acc: 100.00%\n",
      "Val - Epoch: 57 | Loss: 2.6197 | Acc: 47.17%\n",
      "Train - Epoch: 58 | Loss: 0.0017 | Acc: 100.00%\n",
      "Val - Epoch: 58 | Loss: 2.6213 | Acc: 47.17%\n",
      "Train - Epoch: 59 | Loss: 0.0018 | Acc: 100.00%\n",
      "Val - Epoch: 59 | Loss: 2.6227 | Acc: 47.39%\n",
      "Train - Epoch: 60 | Loss: 0.0017 | Acc: 100.00%\n",
      "Val - Epoch: 60 | Loss: 2.6236 | Acc: 47.17%\n",
      "Train - Epoch: 61 | Loss: 0.0017 | Acc: 100.00%\n",
      "Val - Epoch: 61 | Loss: 2.6266 | Acc: 46.96%\n",
      "Train - Epoch: 62 | Loss: 0.0017 | Acc: 100.00%\n",
      "Val - Epoch: 62 | Loss: 2.6281 | Acc: 47.17%\n",
      "Train - Epoch: 63 | Loss: 0.0017 | Acc: 100.00%\n",
      "Val - Epoch: 63 | Loss: 2.6289 | Acc: 47.17%\n",
      "Train - Epoch: 64 | Loss: 0.0017 | Acc: 100.00%\n",
      "Val - Epoch: 64 | Loss: 2.6318 | Acc: 47.17%\n",
      "Train - Epoch: 65 | Loss: 0.0017 | Acc: 100.00%\n",
      "Val - Epoch: 65 | Loss: 2.6328 | Acc: 47.39%\n",
      "Train - Epoch: 66 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 66 | Loss: 2.6342 | Acc: 47.39%\n",
      "Train - Epoch: 67 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 67 | Loss: 2.6347 | Acc: 47.39%\n",
      "Train - Epoch: 68 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 68 | Loss: 2.6360 | Acc: 47.39%\n",
      "Train - Epoch: 69 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 69 | Loss: 2.6370 | Acc: 47.39%\n",
      "Train - Epoch: 70 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 70 | Loss: 2.6388 | Acc: 47.39%\n",
      "Train - Epoch: 71 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 71 | Loss: 2.6406 | Acc: 47.39%\n",
      "Train - Epoch: 72 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 72 | Loss: 2.6415 | Acc: 47.39%\n",
      "Train - Epoch: 73 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 73 | Loss: 2.6432 | Acc: 46.96%\n",
      "Train - Epoch: 74 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 74 | Loss: 2.6436 | Acc: 46.96%\n",
      "Train - Epoch: 75 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 75 | Loss: 2.6438 | Acc: 47.17%\n",
      "Train - Epoch: 76 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 76 | Loss: 2.6448 | Acc: 47.39%\n",
      "Train - Epoch: 77 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 77 | Loss: 2.6455 | Acc: 47.17%\n",
      "Train - Epoch: 78 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 78 | Loss: 2.6459 | Acc: 46.96%\n",
      "Train - Epoch: 79 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 79 | Loss: 2.6466 | Acc: 47.39%\n",
      "Train - Epoch: 80 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 80 | Loss: 2.6472 | Acc: 47.39%\n",
      "Train - Epoch: 81 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 81 | Loss: 2.6480 | Acc: 47.39%\n",
      "Train - Epoch: 82 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 82 | Loss: 2.6487 | Acc: 47.39%\n",
      "Train - Epoch: 83 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 83 | Loss: 2.6496 | Acc: 47.39%\n",
      "Train - Epoch: 84 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 84 | Loss: 2.6499 | Acc: 47.39%\n",
      "Train - Epoch: 85 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 85 | Loss: 2.6505 | Acc: 47.39%\n",
      "Train - Epoch: 86 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 86 | Loss: 2.6511 | Acc: 47.39%\n",
      "Train - Epoch: 87 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 87 | Loss: 2.6522 | Acc: 47.17%\n",
      "Train - Epoch: 88 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 88 | Loss: 2.6527 | Acc: 47.39%\n",
      "Train - Epoch: 89 | Loss: 0.0016 | Acc: 100.00%\n",
      "Val - Epoch: 89 | Loss: 2.6533 | Acc: 47.17%\n",
      "Train - Epoch: 90 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 90 | Loss: 2.6534 | Acc: 47.17%\n",
      "Train - Epoch: 91 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 91 | Loss: 2.6546 | Acc: 47.17%\n",
      "Train - Epoch: 92 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 92 | Loss: 2.6557 | Acc: 47.17%\n",
      "Train - Epoch: 93 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 93 | Loss: 2.6565 | Acc: 47.17%\n",
      "Train - Epoch: 94 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 94 | Loss: 2.6575 | Acc: 47.17%\n",
      "Train - Epoch: 95 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 95 | Loss: 2.6585 | Acc: 47.17%\n",
      "Train - Epoch: 96 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 96 | Loss: 2.6605 | Acc: 46.96%\n",
      "Train - Epoch: 97 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 97 | Loss: 2.6615 | Acc: 47.17%\n",
      "Train - Epoch: 98 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 98 | Loss: 2.6623 | Acc: 47.17%\n",
      "Train - Epoch: 99 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 99 | Loss: 2.6637 | Acc: 47.17%\n",
      "Train - Epoch: 100 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 100 | Loss: 2.6643 | Acc: 46.96%\n",
      "Train - Epoch: 101 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 101 | Loss: 2.6647 | Acc: 47.17%\n",
      "Train - Epoch: 102 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 102 | Loss: 2.6669 | Acc: 46.96%\n",
      "Train - Epoch: 103 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 103 | Loss: 2.6682 | Acc: 46.74%\n",
      "Train - Epoch: 104 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 104 | Loss: 2.6684 | Acc: 46.74%\n",
      "Train - Epoch: 105 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 105 | Loss: 2.6695 | Acc: 46.96%\n",
      "Train - Epoch: 106 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 106 | Loss: 2.6709 | Acc: 46.96%\n",
      "Train - Epoch: 107 | Loss: 0.0015 | Acc: 100.00%\n",
      "Val - Epoch: 107 | Loss: 2.6729 | Acc: 47.17%\n",
      "Train - Epoch: 108 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 108 | Loss: 2.6731 | Acc: 47.17%\n",
      "Train - Epoch: 109 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 109 | Loss: 2.6742 | Acc: 47.17%\n",
      "Train - Epoch: 110 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 110 | Loss: 2.6758 | Acc: 47.17%\n",
      "Train - Epoch: 111 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 111 | Loss: 2.6769 | Acc: 46.96%\n",
      "Train - Epoch: 112 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 112 | Loss: 2.6779 | Acc: 47.17%\n",
      "Train - Epoch: 113 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 113 | Loss: 2.6787 | Acc: 47.17%\n",
      "Train - Epoch: 114 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 114 | Loss: 2.6806 | Acc: 47.17%\n",
      "Train - Epoch: 115 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 115 | Loss: 2.6819 | Acc: 47.17%\n",
      "Train - Epoch: 116 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 116 | Loss: 2.6828 | Acc: 47.17%\n",
      "Train - Epoch: 117 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 117 | Loss: 2.6841 | Acc: 47.17%\n",
      "Train - Epoch: 118 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 118 | Loss: 2.6873 | Acc: 47.17%\n",
      "Train - Epoch: 119 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 119 | Loss: 2.6885 | Acc: 47.17%\n",
      "Train - Epoch: 120 | Loss: 0.0014 | Acc: 100.00%\n",
      "Val - Epoch: 120 | Loss: 2.6901 | Acc: 47.17%\n",
      "Train - Epoch: 121 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 121 | Loss: 2.6931 | Acc: 47.17%\n",
      "Train - Epoch: 122 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 122 | Loss: 2.6938 | Acc: 47.17%\n",
      "Train - Epoch: 123 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 123 | Loss: 2.6958 | Acc: 46.96%\n",
      "Train - Epoch: 124 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 124 | Loss: 2.6970 | Acc: 47.17%\n",
      "Train - Epoch: 125 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 125 | Loss: 2.6994 | Acc: 47.17%\n",
      "Train - Epoch: 126 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 126 | Loss: 2.7008 | Acc: 46.96%\n",
      "Train - Epoch: 127 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 127 | Loss: 2.7019 | Acc: 46.96%\n",
      "Train - Epoch: 128 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 128 | Loss: 2.7051 | Acc: 46.74%\n",
      "Train - Epoch: 129 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 129 | Loss: 2.7065 | Acc: 46.74%\n",
      "Train - Epoch: 130 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 130 | Loss: 2.7073 | Acc: 46.96%\n",
      "Train - Epoch: 131 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 131 | Loss: 2.7092 | Acc: 47.17%\n",
      "Train - Epoch: 132 | Loss: 0.0012 | Acc: 100.00%\n",
      "Val - Epoch: 132 | Loss: 2.7115 | Acc: 47.17%\n",
      "Train - Epoch: 133 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 133 | Loss: 2.7143 | Acc: 46.96%\n",
      "Train - Epoch: 134 | Loss: 0.0012 | Acc: 100.00%\n",
      "Val - Epoch: 134 | Loss: 2.7167 | Acc: 47.17%\n",
      "Train - Epoch: 135 | Loss: 0.0012 | Acc: 100.00%\n",
      "Val - Epoch: 135 | Loss: 2.7188 | Acc: 47.17%\n",
      "Train - Epoch: 136 | Loss: 0.0012 | Acc: 100.00%\n",
      "Val - Epoch: 136 | Loss: 2.7204 | Acc: 47.17%\n",
      "Train - Epoch: 137 | Loss: 0.0013 | Acc: 100.00%\n",
      "Val - Epoch: 137 | Loss: 2.7245 | Acc: 46.74%\n",
      "Train - Epoch: 138 | Loss: 0.0012 | Acc: 100.00%\n",
      "Val - Epoch: 138 | Loss: 2.7251 | Acc: 47.17%\n",
      "Train - Epoch: 139 | Loss: 0.0012 | Acc: 100.00%\n",
      "Val - Epoch: 139 | Loss: 2.7277 | Acc: 47.17%\n",
      "Train - Epoch: 140 | Loss: 0.0012 | Acc: 100.00%\n",
      "Val - Epoch: 140 | Loss: 2.7301 | Acc: 47.17%\n",
      "Train - Epoch: 141 | Loss: 0.0012 | Acc: 100.00%\n",
      "Val - Epoch: 141 | Loss: 2.7322 | Acc: 47.17%\n",
      "Train - Epoch: 142 | Loss: 0.0011 | Acc: 100.00%\n",
      "Val - Epoch: 142 | Loss: 2.7339 | Acc: 46.96%\n",
      "Train - Epoch: 143 | Loss: 0.0012 | Acc: 100.00%\n",
      "Val - Epoch: 143 | Loss: 2.7373 | Acc: 46.96%\n",
      "Train - Epoch: 144 | Loss: 0.0012 | Acc: 100.00%\n",
      "Val - Epoch: 144 | Loss: 2.7385 | Acc: 46.96%\n",
      "Train - Epoch: 145 | Loss: 0.0012 | Acc: 100.00%\n",
      "Val - Epoch: 145 | Loss: 2.7411 | Acc: 46.96%\n",
      "Train - Epoch: 146 | Loss: 0.0011 | Acc: 100.00%\n",
      "Val - Epoch: 146 | Loss: 2.7452 | Acc: 46.96%\n",
      "Train - Epoch: 147 | Loss: 0.0011 | Acc: 100.00%\n",
      "Val - Epoch: 147 | Loss: 2.7464 | Acc: 47.17%\n",
      "Train - Epoch: 148 | Loss: 0.0011 | Acc: 100.00%\n",
      "Val - Epoch: 148 | Loss: 2.7507 | Acc: 46.96%\n",
      "Train - Epoch: 149 | Loss: 0.0011 | Acc: 100.00%\n",
      "Val - Epoch: 149 | Loss: 2.7525 | Acc: 46.96%\n",
      "Train - Epoch: 150 | Loss: 0.0011 | Acc: 100.00%\n",
      "Val - Epoch: 150 | Loss: 2.7557 | Acc: 47.17%\n",
      "Train - Epoch: 151 | Loss: 0.0011 | Acc: 100.00%\n",
      "Val - Epoch: 151 | Loss: 2.7591 | Acc: 47.17%\n",
      "Train - Epoch: 152 | Loss: 0.0011 | Acc: 100.00%\n",
      "Val - Epoch: 152 | Loss: 2.7610 | Acc: 46.96%\n",
      "Train - Epoch: 153 | Loss: 0.0011 | Acc: 100.00%\n",
      "Val - Epoch: 153 | Loss: 2.7621 | Acc: 47.17%\n",
      "Train - Epoch: 154 | Loss: 0.0011 | Acc: 100.00%\n",
      "Val - Epoch: 154 | Loss: 2.7665 | Acc: 47.17%\n",
      "Train - Epoch: 155 | Loss: 0.0011 | Acc: 100.00%\n",
      "Val - Epoch: 155 | Loss: 2.7705 | Acc: 47.17%\n",
      "Train - Epoch: 156 | Loss: 0.0010 | Acc: 100.00%\n",
      "Val - Epoch: 156 | Loss: 2.7723 | Acc: 47.17%\n",
      "Train - Epoch: 157 | Loss: 0.0010 | Acc: 100.00%\n",
      "Val - Epoch: 157 | Loss: 2.7747 | Acc: 46.96%\n",
      "Train - Epoch: 158 | Loss: 0.0010 | Acc: 100.00%\n",
      "Val - Epoch: 158 | Loss: 2.7780 | Acc: 47.17%\n",
      "Train - Epoch: 159 | Loss: 0.0010 | Acc: 100.00%\n",
      "Val - Epoch: 159 | Loss: 2.7817 | Acc: 47.17%\n",
      "Train - Epoch: 160 | Loss: 0.0010 | Acc: 100.00%\n",
      "Val - Epoch: 160 | Loss: 2.7840 | Acc: 47.17%\n",
      "Train - Epoch: 161 | Loss: 0.0010 | Acc: 100.00%\n",
      "Val - Epoch: 161 | Loss: 2.7877 | Acc: 46.96%\n",
      "Train - Epoch: 162 | Loss: 0.0010 | Acc: 100.00%\n",
      "Val - Epoch: 162 | Loss: 2.7912 | Acc: 47.17%\n",
      "Train - Epoch: 163 | Loss: 0.0010 | Acc: 100.00%\n",
      "Val - Epoch: 163 | Loss: 2.7924 | Acc: 47.17%\n",
      "Train - Epoch: 164 | Loss: 0.0010 | Acc: 100.00%\n",
      "Val - Epoch: 164 | Loss: 2.7977 | Acc: 46.96%\n",
      "Train - Epoch: 165 | Loss: 0.0009 | Acc: 100.00%\n",
      "Val - Epoch: 165 | Loss: 2.8005 | Acc: 47.17%\n",
      "Train - Epoch: 166 | Loss: 0.0009 | Acc: 100.00%\n",
      "Val - Epoch: 166 | Loss: 2.8048 | Acc: 46.96%\n",
      "Train - Epoch: 167 | Loss: 0.0009 | Acc: 100.00%\n",
      "Val - Epoch: 167 | Loss: 2.8054 | Acc: 47.17%\n",
      "Train - Epoch: 168 | Loss: 0.0009 | Acc: 100.00%\n",
      "Val - Epoch: 168 | Loss: 2.8121 | Acc: 46.74%\n",
      "Train - Epoch: 169 | Loss: 0.0009 | Acc: 100.00%\n",
      "Val - Epoch: 169 | Loss: 2.8147 | Acc: 46.52%\n",
      "Train - Epoch: 170 | Loss: 0.0009 | Acc: 100.00%\n",
      "Val - Epoch: 170 | Loss: 2.8165 | Acc: 47.17%\n",
      "Train - Epoch: 171 | Loss: 0.0009 | Acc: 100.00%\n",
      "Val - Epoch: 171 | Loss: 2.8217 | Acc: 47.17%\n",
      "Train - Epoch: 172 | Loss: 0.0009 | Acc: 100.00%\n",
      "Val - Epoch: 172 | Loss: 2.8240 | Acc: 46.96%\n",
      "Train - Epoch: 173 | Loss: 0.0009 | Acc: 100.00%\n",
      "Val - Epoch: 173 | Loss: 2.8288 | Acc: 46.74%\n",
      "Train - Epoch: 174 | Loss: 0.0009 | Acc: 100.00%\n",
      "Val - Epoch: 174 | Loss: 2.8351 | Acc: 46.74%\n",
      "Train - Epoch: 175 | Loss: 0.0009 | Acc: 100.00%\n",
      "Val - Epoch: 175 | Loss: 2.8355 | Acc: 46.74%\n",
      "Train - Epoch: 176 | Loss: 0.0008 | Acc: 100.00%\n",
      "Val - Epoch: 176 | Loss: 2.8407 | Acc: 46.52%\n",
      "Train - Epoch: 177 | Loss: 0.0008 | Acc: 100.00%\n",
      "Val - Epoch: 177 | Loss: 2.8452 | Acc: 46.52%\n",
      "Train - Epoch: 178 | Loss: 0.0008 | Acc: 100.00%\n",
      "Val - Epoch: 178 | Loss: 2.8484 | Acc: 46.52%\n",
      "Train - Epoch: 179 | Loss: 0.0008 | Acc: 100.00%\n",
      "Val - Epoch: 179 | Loss: 2.8515 | Acc: 46.74%\n",
      "Train - Epoch: 180 | Loss: 0.0008 | Acc: 100.00%\n",
      "Val - Epoch: 180 | Loss: 2.8544 | Acc: 46.30%\n",
      "Train - Epoch: 181 | Loss: 0.0008 | Acc: 100.00%\n",
      "Val - Epoch: 181 | Loss: 2.8597 | Acc: 46.30%\n",
      "Train - Epoch: 182 | Loss: 0.0008 | Acc: 100.00%\n",
      "Val - Epoch: 182 | Loss: 2.8627 | Acc: 46.74%\n",
      "Train - Epoch: 183 | Loss: 0.0008 | Acc: 100.00%\n",
      "Val - Epoch: 183 | Loss: 2.8671 | Acc: 46.52%\n",
      "Train - Epoch: 184 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 184 | Loss: 2.8686 | Acc: 46.30%\n",
      "Train - Epoch: 185 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 185 | Loss: 2.8737 | Acc: 46.30%\n",
      "Train - Epoch: 186 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 186 | Loss: 2.8766 | Acc: 46.09%\n",
      "Train - Epoch: 187 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 187 | Loss: 2.8831 | Acc: 46.09%\n",
      "Train - Epoch: 188 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 188 | Loss: 2.8869 | Acc: 46.30%\n",
      "Train - Epoch: 189 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 189 | Loss: 2.8941 | Acc: 46.09%\n",
      "Train - Epoch: 190 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 190 | Loss: 2.8924 | Acc: 46.30%\n",
      "Train - Epoch: 191 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 191 | Loss: 2.8962 | Acc: 45.87%\n",
      "Train - Epoch: 192 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 192 | Loss: 2.9005 | Acc: 46.30%\n",
      "Train - Epoch: 193 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 193 | Loss: 2.9043 | Acc: 46.30%\n",
      "Train - Epoch: 194 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 194 | Loss: 2.9109 | Acc: 46.52%\n",
      "Train - Epoch: 195 | Loss: 0.0006 | Acc: 100.00%\n",
      "Val - Epoch: 195 | Loss: 2.9162 | Acc: 46.09%\n",
      "Train - Epoch: 196 | Loss: 0.0007 | Acc: 100.00%\n",
      "Val - Epoch: 196 | Loss: 2.9211 | Acc: 45.87%\n",
      "Train - Epoch: 197 | Loss: 0.0006 | Acc: 100.00%\n",
      "Val - Epoch: 197 | Loss: 2.9233 | Acc: 46.30%\n",
      "Train - Epoch: 198 | Loss: 0.0006 | Acc: 100.00%\n",
      "Val - Epoch: 198 | Loss: 2.9280 | Acc: 46.30%\n",
      "Train - Epoch: 199 | Loss: 0.0006 | Acc: 100.00%\n",
      "Val - Epoch: 199 | Loss: 2.9360 | Acc: 45.65%\n",
      "Train - Epoch: 200 | Loss: 0.0006 | Acc: 100.00%\n",
      "Val - Epoch: 200 | Loss: 2.9414 | Acc: 46.09%\n",
      "Train - Epoch: 201 | Loss: 0.0006 | Acc: 100.00%\n",
      "Val - Epoch: 201 | Loss: 2.9436 | Acc: 45.87%\n",
      "Train - Epoch: 202 | Loss: 0.0006 | Acc: 100.00%\n",
      "Val - Epoch: 202 | Loss: 2.9488 | Acc: 45.87%\n",
      "Train - Epoch: 203 | Loss: 0.0006 | Acc: 100.00%\n",
      "Val - Epoch: 203 | Loss: 2.9531 | Acc: 45.87%\n",
      "Train - Epoch: 204 | Loss: 0.0006 | Acc: 100.00%\n",
      "Val - Epoch: 204 | Loss: 2.9550 | Acc: 45.65%\n",
      "Train - Epoch: 205 | Loss: 0.0006 | Acc: 100.00%\n",
      "Val - Epoch: 205 | Loss: 2.9603 | Acc: 45.65%\n",
      "Train - Epoch: 206 | Loss: 0.0006 | Acc: 100.00%\n",
      "Val - Epoch: 206 | Loss: 2.9643 | Acc: 45.65%\n",
      "Train - Epoch: 207 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 207 | Loss: 2.9702 | Acc: 45.65%\n",
      "Train - Epoch: 208 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 208 | Loss: 2.9740 | Acc: 45.43%\n",
      "Train - Epoch: 209 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 209 | Loss: 2.9794 | Acc: 45.43%\n",
      "Train - Epoch: 210 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 210 | Loss: 2.9863 | Acc: 45.65%\n",
      "Train - Epoch: 211 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 211 | Loss: 2.9901 | Acc: 45.87%\n",
      "Train - Epoch: 212 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 212 | Loss: 2.9938 | Acc: 45.87%\n",
      "Train - Epoch: 213 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 213 | Loss: 2.9974 | Acc: 45.22%\n",
      "Train - Epoch: 214 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 214 | Loss: 3.0025 | Acc: 45.00%\n",
      "Train - Epoch: 215 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 215 | Loss: 3.0085 | Acc: 45.43%\n",
      "Train - Epoch: 216 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 216 | Loss: 3.0108 | Acc: 45.22%\n",
      "Train - Epoch: 217 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 217 | Loss: 3.0178 | Acc: 45.65%\n",
      "Train - Epoch: 218 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 218 | Loss: 3.0187 | Acc: 45.65%\n",
      "Train - Epoch: 219 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 219 | Loss: 3.0273 | Acc: 45.43%\n",
      "Train - Epoch: 220 | Loss: 0.0005 | Acc: 100.00%\n",
      "Val - Epoch: 220 | Loss: 3.0297 | Acc: 45.00%\n",
      "Train - Epoch: 221 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 221 | Loss: 3.0352 | Acc: 45.43%\n",
      "Train - Epoch: 222 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 222 | Loss: 3.0398 | Acc: 45.65%\n",
      "Train - Epoch: 223 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 223 | Loss: 3.0464 | Acc: 45.22%\n",
      "Train - Epoch: 224 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 224 | Loss: 3.0537 | Acc: 45.22%\n",
      "Train - Epoch: 225 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 225 | Loss: 3.0595 | Acc: 45.22%\n",
      "Train - Epoch: 226 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 226 | Loss: 3.0597 | Acc: 45.00%\n",
      "Train - Epoch: 227 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 227 | Loss: 3.0632 | Acc: 45.00%\n",
      "Train - Epoch: 228 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 228 | Loss: 3.0699 | Acc: 45.00%\n",
      "Train - Epoch: 229 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 229 | Loss: 3.0729 | Acc: 45.22%\n",
      "Train - Epoch: 230 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 230 | Loss: 3.0817 | Acc: 45.00%\n",
      "Train - Epoch: 231 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 231 | Loss: 3.0872 | Acc: 44.78%\n",
      "Train - Epoch: 232 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 232 | Loss: 3.0940 | Acc: 44.78%\n",
      "Train - Epoch: 233 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 233 | Loss: 3.0954 | Acc: 44.78%\n",
      "Train - Epoch: 234 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 234 | Loss: 3.1035 | Acc: 44.78%\n",
      "Train - Epoch: 235 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 235 | Loss: 3.1044 | Acc: 45.00%\n",
      "Train - Epoch: 236 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 236 | Loss: 3.1144 | Acc: 44.78%\n",
      "Train - Epoch: 237 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 237 | Loss: 3.1188 | Acc: 45.00%\n",
      "Train - Epoch: 238 | Loss: 0.0004 | Acc: 100.00%\n",
      "Val - Epoch: 238 | Loss: 3.1204 | Acc: 45.00%\n",
      "Train - Epoch: 239 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 239 | Loss: 3.1302 | Acc: 44.57%\n",
      "Train - Epoch: 240 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 240 | Loss: 3.1354 | Acc: 44.57%\n",
      "Train - Epoch: 241 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 241 | Loss: 3.1349 | Acc: 44.78%\n",
      "Train - Epoch: 242 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 242 | Loss: 3.1426 | Acc: 45.00%\n",
      "Train - Epoch: 243 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 243 | Loss: 3.1478 | Acc: 45.00%\n",
      "Train - Epoch: 244 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 244 | Loss: 3.1540 | Acc: 44.78%\n",
      "Train - Epoch: 245 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 245 | Loss: 3.1591 | Acc: 44.78%\n",
      "Train - Epoch: 246 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 246 | Loss: 3.1626 | Acc: 45.00%\n",
      "Train - Epoch: 247 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 247 | Loss: 3.1674 | Acc: 45.00%\n",
      "Train - Epoch: 248 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 248 | Loss: 3.1720 | Acc: 45.00%\n",
      "Train - Epoch: 249 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 249 | Loss: 3.1810 | Acc: 45.00%\n",
      "Train - Epoch: 250 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 250 | Loss: 3.1871 | Acc: 45.00%\n",
      "Train - Epoch: 251 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 251 | Loss: 3.1927 | Acc: 44.78%\n",
      "Train - Epoch: 252 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 252 | Loss: 3.2004 | Acc: 44.57%\n",
      "Train - Epoch: 253 | Loss: 0.0003 | Acc: 100.00%\n",
      "Val - Epoch: 253 | Loss: 3.2071 | Acc: 44.78%\n",
      "Train - Epoch: 254 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 254 | Loss: 3.2121 | Acc: 44.78%\n",
      "Train - Epoch: 255 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 255 | Loss: 3.2174 | Acc: 44.78%\n",
      "Train - Epoch: 256 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 256 | Loss: 3.2209 | Acc: 44.57%\n",
      "Train - Epoch: 257 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 257 | Loss: 3.2260 | Acc: 44.78%\n",
      "Train - Epoch: 258 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 258 | Loss: 3.2332 | Acc: 44.78%\n",
      "Train - Epoch: 259 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 259 | Loss: 3.2363 | Acc: 45.00%\n",
      "Train - Epoch: 260 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 260 | Loss: 3.2429 | Acc: 45.00%\n",
      "Train - Epoch: 261 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 261 | Loss: 3.2518 | Acc: 44.78%\n",
      "Train - Epoch: 262 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 262 | Loss: 3.2550 | Acc: 44.35%\n",
      "Train - Epoch: 263 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 263 | Loss: 3.2591 | Acc: 44.78%\n",
      "Train - Epoch: 264 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 264 | Loss: 3.2667 | Acc: 44.78%\n",
      "Train - Epoch: 265 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 265 | Loss: 3.2745 | Acc: 44.57%\n",
      "Train - Epoch: 266 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 266 | Loss: 3.2804 | Acc: 44.57%\n",
      "Train - Epoch: 267 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 267 | Loss: 3.2836 | Acc: 45.00%\n",
      "Train - Epoch: 268 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 268 | Loss: 3.2894 | Acc: 44.57%\n",
      "Train - Epoch: 269 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 269 | Loss: 3.2962 | Acc: 44.13%\n",
      "Train - Epoch: 270 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 270 | Loss: 3.3057 | Acc: 44.78%\n",
      "Train - Epoch: 271 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 271 | Loss: 3.3066 | Acc: 44.78%\n",
      "Train - Epoch: 272 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 272 | Loss: 3.3131 | Acc: 44.35%\n",
      "Train - Epoch: 273 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 273 | Loss: 3.3186 | Acc: 44.57%\n",
      "Train - Epoch: 274 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 274 | Loss: 3.3253 | Acc: 44.78%\n",
      "Train - Epoch: 275 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 275 | Loss: 3.3307 | Acc: 44.35%\n",
      "Train - Epoch: 276 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 276 | Loss: 3.3341 | Acc: 45.00%\n",
      "Train - Epoch: 277 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 277 | Loss: 3.3418 | Acc: 44.78%\n",
      "Train - Epoch: 278 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 278 | Loss: 3.3474 | Acc: 44.35%\n",
      "Train - Epoch: 279 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 279 | Loss: 3.3541 | Acc: 44.57%\n",
      "Train - Epoch: 280 | Loss: 0.0002 | Acc: 100.00%\n",
      "Val - Epoch: 280 | Loss: 3.3610 | Acc: 44.78%\n",
      "Train - Epoch: 281 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 281 | Loss: 3.3664 | Acc: 44.35%\n",
      "Train - Epoch: 282 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 282 | Loss: 3.3756 | Acc: 44.57%\n",
      "Train - Epoch: 283 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 283 | Loss: 3.3757 | Acc: 44.78%\n",
      "Train - Epoch: 284 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 284 | Loss: 3.3813 | Acc: 44.57%\n",
      "Train - Epoch: 285 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 285 | Loss: 3.3935 | Acc: 44.57%\n",
      "Train - Epoch: 286 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 286 | Loss: 3.4033 | Acc: 43.91%\n",
      "Train - Epoch: 287 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 287 | Loss: 3.4051 | Acc: 44.57%\n",
      "Train - Epoch: 288 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 288 | Loss: 3.4101 | Acc: 44.35%\n",
      "Train - Epoch: 289 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 289 | Loss: 3.4116 | Acc: 44.78%\n",
      "Train - Epoch: 290 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 290 | Loss: 3.4218 | Acc: 44.35%\n",
      "Train - Epoch: 291 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 291 | Loss: 3.4309 | Acc: 44.78%\n",
      "Train - Epoch: 292 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 292 | Loss: 3.4337 | Acc: 44.35%\n",
      "Train - Epoch: 293 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 293 | Loss: 3.4401 | Acc: 44.57%\n",
      "Train - Epoch: 294 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 294 | Loss: 3.4446 | Acc: 44.57%\n",
      "Train - Epoch: 295 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 295 | Loss: 3.4566 | Acc: 44.35%\n",
      "Train - Epoch: 296 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 296 | Loss: 3.4569 | Acc: 44.35%\n",
      "Train - Epoch: 297 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 297 | Loss: 3.4700 | Acc: 44.57%\n",
      "Train - Epoch: 298 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 298 | Loss: 3.4718 | Acc: 44.35%\n",
      "Train - Epoch: 299 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 299 | Loss: 3.4762 | Acc: 44.35%\n",
      "Train - Epoch: 300 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 300 | Loss: 3.4815 | Acc: 44.13%\n",
      "Train - Epoch: 301 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 301 | Loss: 3.4870 | Acc: 44.13%\n",
      "Train - Epoch: 302 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 302 | Loss: 3.4950 | Acc: 45.00%\n",
      "Train - Epoch: 303 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 303 | Loss: 3.5006 | Acc: 44.35%\n",
      "Train - Epoch: 304 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 304 | Loss: 3.5083 | Acc: 44.35%\n",
      "Train - Epoch: 305 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 305 | Loss: 3.5097 | Acc: 44.57%\n",
      "Train - Epoch: 306 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 306 | Loss: 3.5196 | Acc: 44.13%\n",
      "Train - Epoch: 307 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 307 | Loss: 3.5215 | Acc: 43.91%\n",
      "Train - Epoch: 308 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 308 | Loss: 3.5298 | Acc: 44.35%\n",
      "Train - Epoch: 309 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 309 | Loss: 3.5410 | Acc: 43.91%\n",
      "Train - Epoch: 310 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 310 | Loss: 3.5407 | Acc: 44.57%\n",
      "Train - Epoch: 311 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 311 | Loss: 3.5529 | Acc: 44.35%\n",
      "Train - Epoch: 312 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 312 | Loss: 3.5545 | Acc: 44.13%\n",
      "Train - Epoch: 313 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 313 | Loss: 3.5653 | Acc: 44.13%\n",
      "Train - Epoch: 314 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 314 | Loss: 3.5671 | Acc: 44.35%\n",
      "Train - Epoch: 315 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 315 | Loss: 3.5725 | Acc: 43.91%\n",
      "Train - Epoch: 316 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 316 | Loss: 3.5805 | Acc: 44.13%\n",
      "Train - Epoch: 317 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 317 | Loss: 3.5884 | Acc: 43.91%\n",
      "Train - Epoch: 318 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 318 | Loss: 3.5943 | Acc: 44.35%\n",
      "Train - Epoch: 319 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 319 | Loss: 3.5994 | Acc: 43.91%\n",
      "Train - Epoch: 320 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 320 | Loss: 3.6075 | Acc: 43.91%\n",
      "Train - Epoch: 321 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 321 | Loss: 3.6150 | Acc: 44.13%\n",
      "Train - Epoch: 322 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 322 | Loss: 3.6204 | Acc: 43.70%\n",
      "Train - Epoch: 323 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 323 | Loss: 3.6248 | Acc: 43.70%\n",
      "Train - Epoch: 324 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 324 | Loss: 3.6333 | Acc: 43.91%\n",
      "Train - Epoch: 325 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 325 | Loss: 3.6408 | Acc: 43.70%\n",
      "Train - Epoch: 326 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 326 | Loss: 3.6467 | Acc: 43.48%\n",
      "Train - Epoch: 327 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 327 | Loss: 3.6553 | Acc: 43.91%\n",
      "Train - Epoch: 328 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 328 | Loss: 3.6563 | Acc: 43.70%\n",
      "Train - Epoch: 329 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 329 | Loss: 3.6610 | Acc: 43.48%\n",
      "Train - Epoch: 330 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 330 | Loss: 3.6743 | Acc: 43.70%\n",
      "Train - Epoch: 331 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 331 | Loss: 3.6796 | Acc: 43.91%\n",
      "Train - Epoch: 332 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 332 | Loss: 3.6912 | Acc: 43.48%\n",
      "Train - Epoch: 333 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 333 | Loss: 3.6908 | Acc: 43.91%\n",
      "Train - Epoch: 334 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 334 | Loss: 3.6992 | Acc: 43.04%\n",
      "Train - Epoch: 335 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 335 | Loss: 3.7023 | Acc: 43.26%\n",
      "Train - Epoch: 336 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 336 | Loss: 3.7064 | Acc: 43.26%\n",
      "Train - Epoch: 337 | Loss: 0.0001 | Acc: 100.00%\n",
      "Val - Epoch: 337 | Loss: 3.7182 | Acc: 43.26%\n",
      "Train - Epoch: 338 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 338 | Loss: 3.7184 | Acc: 43.04%\n",
      "Train - Epoch: 339 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 339 | Loss: 3.7260 | Acc: 43.26%\n",
      "Train - Epoch: 340 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 340 | Loss: 3.7360 | Acc: 43.48%\n",
      "Train - Epoch: 341 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 341 | Loss: 3.7391 | Acc: 43.48%\n",
      "Train - Epoch: 342 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 342 | Loss: 3.7470 | Acc: 43.26%\n",
      "Train - Epoch: 343 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 343 | Loss: 3.7487 | Acc: 42.83%\n",
      "Train - Epoch: 344 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 344 | Loss: 3.7557 | Acc: 42.61%\n",
      "Train - Epoch: 345 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 345 | Loss: 3.7651 | Acc: 43.26%\n",
      "Train - Epoch: 346 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 346 | Loss: 3.7691 | Acc: 43.26%\n",
      "Train - Epoch: 347 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 347 | Loss: 3.7809 | Acc: 43.48%\n",
      "Train - Epoch: 348 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 348 | Loss: 3.7827 | Acc: 43.26%\n",
      "Train - Epoch: 349 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 349 | Loss: 3.7884 | Acc: 43.48%\n",
      "Train - Epoch: 350 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 350 | Loss: 3.7970 | Acc: 43.26%\n",
      "Train - Epoch: 351 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 351 | Loss: 3.8050 | Acc: 43.48%\n",
      "Train - Epoch: 352 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 352 | Loss: 3.8137 | Acc: 43.04%\n",
      "Train - Epoch: 353 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 353 | Loss: 3.8188 | Acc: 43.26%\n",
      "Train - Epoch: 354 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 354 | Loss: 3.8274 | Acc: 43.26%\n",
      "Train - Epoch: 355 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 355 | Loss: 3.8319 | Acc: 43.48%\n",
      "Train - Epoch: 356 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 356 | Loss: 3.8340 | Acc: 43.04%\n",
      "Train - Epoch: 357 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 357 | Loss: 3.8417 | Acc: 43.26%\n",
      "Train - Epoch: 358 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 358 | Loss: 3.8451 | Acc: 43.04%\n",
      "Train - Epoch: 359 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 359 | Loss: 3.8586 | Acc: 43.48%\n",
      "Train - Epoch: 360 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 360 | Loss: 3.8655 | Acc: 43.26%\n",
      "Train - Epoch: 361 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 361 | Loss: 3.8689 | Acc: 43.26%\n",
      "Train - Epoch: 362 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 362 | Loss: 3.8725 | Acc: 43.26%\n",
      "Train - Epoch: 363 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 363 | Loss: 3.8779 | Acc: 43.26%\n",
      "Train - Epoch: 364 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 364 | Loss: 3.8856 | Acc: 43.48%\n",
      "Train - Epoch: 365 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 365 | Loss: 3.8908 | Acc: 43.48%\n",
      "Train - Epoch: 366 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 366 | Loss: 3.9006 | Acc: 43.70%\n",
      "Train - Epoch: 367 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 367 | Loss: 3.9091 | Acc: 43.48%\n",
      "Train - Epoch: 368 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 368 | Loss: 3.9146 | Acc: 43.48%\n",
      "Train - Epoch: 369 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 369 | Loss: 3.9240 | Acc: 43.70%\n",
      "Train - Epoch: 370 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 370 | Loss: 3.9277 | Acc: 43.48%\n",
      "Train - Epoch: 371 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 371 | Loss: 3.9355 | Acc: 43.26%\n",
      "Train - Epoch: 372 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 372 | Loss: 3.9478 | Acc: 43.48%\n",
      "Train - Epoch: 373 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 373 | Loss: 3.9517 | Acc: 43.48%\n",
      "Train - Epoch: 374 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 374 | Loss: 3.9574 | Acc: 43.04%\n",
      "Train - Epoch: 375 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 375 | Loss: 3.9598 | Acc: 43.48%\n",
      "Train - Epoch: 376 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 376 | Loss: 3.9671 | Acc: 43.26%\n",
      "Train - Epoch: 377 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 377 | Loss: 3.9740 | Acc: 42.83%\n",
      "Train - Epoch: 378 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 378 | Loss: 3.9822 | Acc: 43.26%\n",
      "Train - Epoch: 379 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 379 | Loss: 3.9927 | Acc: 43.48%\n",
      "Train - Epoch: 380 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 380 | Loss: 4.0007 | Acc: 43.91%\n",
      "Train - Epoch: 381 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 381 | Loss: 3.9984 | Acc: 43.26%\n",
      "Train - Epoch: 382 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 382 | Loss: 4.0106 | Acc: 43.48%\n",
      "Train - Epoch: 383 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 383 | Loss: 4.0169 | Acc: 43.04%\n",
      "Train - Epoch: 384 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 384 | Loss: 4.0266 | Acc: 43.26%\n",
      "Train - Epoch: 385 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 385 | Loss: 4.0248 | Acc: 43.26%\n",
      "Train - Epoch: 386 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 386 | Loss: 4.0353 | Acc: 43.48%\n",
      "Train - Epoch: 387 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 387 | Loss: 4.0419 | Acc: 43.48%\n",
      "Train - Epoch: 388 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 388 | Loss: 4.0485 | Acc: 43.48%\n",
      "Train - Epoch: 389 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 389 | Loss: 4.0527 | Acc: 43.26%\n",
      "Train - Epoch: 390 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 390 | Loss: 4.0679 | Acc: 43.48%\n",
      "Train - Epoch: 391 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 391 | Loss: 4.0761 | Acc: 43.70%\n",
      "Train - Epoch: 392 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 392 | Loss: 4.0785 | Acc: 43.48%\n",
      "Train - Epoch: 393 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 393 | Loss: 4.0838 | Acc: 43.26%\n",
      "Train - Epoch: 394 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 394 | Loss: 4.0865 | Acc: 43.26%\n",
      "Train - Epoch: 395 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 395 | Loss: 4.0964 | Acc: 43.48%\n",
      "Train - Epoch: 396 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 396 | Loss: 4.1007 | Acc: 43.26%\n",
      "Train - Epoch: 397 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 397 | Loss: 4.1113 | Acc: 42.83%\n",
      "Train - Epoch: 398 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 398 | Loss: 4.1214 | Acc: 42.83%\n",
      "Train - Epoch: 399 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 399 | Loss: 4.1237 | Acc: 43.26%\n",
      "Train - Epoch: 400 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 400 | Loss: 4.1337 | Acc: 43.26%\n",
      "Train - Epoch: 401 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 401 | Loss: 4.1356 | Acc: 43.26%\n",
      "Train - Epoch: 402 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 402 | Loss: 4.1485 | Acc: 43.70%\n",
      "Train - Epoch: 403 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 403 | Loss: 4.1495 | Acc: 43.26%\n",
      "Train - Epoch: 404 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 404 | Loss: 4.1530 | Acc: 43.26%\n",
      "Train - Epoch: 405 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 405 | Loss: 4.1686 | Acc: 43.70%\n",
      "Train - Epoch: 406 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 406 | Loss: 4.1743 | Acc: 43.48%\n",
      "Train - Epoch: 407 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 407 | Loss: 4.1808 | Acc: 43.70%\n",
      "Train - Epoch: 408 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 408 | Loss: 4.1827 | Acc: 43.04%\n",
      "Train - Epoch: 409 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 409 | Loss: 4.1962 | Acc: 43.04%\n",
      "Train - Epoch: 410 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 410 | Loss: 4.1987 | Acc: 43.04%\n",
      "Train - Epoch: 411 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 411 | Loss: 4.2088 | Acc: 43.04%\n",
      "Train - Epoch: 412 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 412 | Loss: 4.2109 | Acc: 43.26%\n",
      "Train - Epoch: 413 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 413 | Loss: 4.2165 | Acc: 43.26%\n",
      "Train - Epoch: 414 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 414 | Loss: 4.2263 | Acc: 43.04%\n",
      "Train - Epoch: 415 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 415 | Loss: 4.2263 | Acc: 43.04%\n",
      "Train - Epoch: 416 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 416 | Loss: 4.2381 | Acc: 42.83%\n",
      "Train - Epoch: 417 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 417 | Loss: 4.2454 | Acc: 43.04%\n",
      "Train - Epoch: 418 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 418 | Loss: 4.2510 | Acc: 43.48%\n",
      "Train - Epoch: 419 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 419 | Loss: 4.2622 | Acc: 43.26%\n",
      "Train - Epoch: 420 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 420 | Loss: 4.2660 | Acc: 43.48%\n",
      "Train - Epoch: 421 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 421 | Loss: 4.2751 | Acc: 43.26%\n",
      "Train - Epoch: 422 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 422 | Loss: 4.2901 | Acc: 43.26%\n",
      "Train - Epoch: 423 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 423 | Loss: 4.2882 | Acc: 43.48%\n",
      "Train - Epoch: 424 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 424 | Loss: 4.2993 | Acc: 43.26%\n",
      "Train - Epoch: 425 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 425 | Loss: 4.3014 | Acc: 43.26%\n",
      "Train - Epoch: 426 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 426 | Loss: 4.3058 | Acc: 43.04%\n",
      "Train - Epoch: 427 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 427 | Loss: 4.3096 | Acc: 42.61%\n",
      "Train - Epoch: 428 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 428 | Loss: 4.3217 | Acc: 43.04%\n",
      "Train - Epoch: 429 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 429 | Loss: 4.3261 | Acc: 43.26%\n",
      "Train - Epoch: 430 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 430 | Loss: 4.3420 | Acc: 43.26%\n",
      "Train - Epoch: 431 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 431 | Loss: 4.3471 | Acc: 43.48%\n",
      "Train - Epoch: 432 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 432 | Loss: 4.3546 | Acc: 43.26%\n",
      "Train - Epoch: 433 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 433 | Loss: 4.3603 | Acc: 43.26%\n",
      "Train - Epoch: 434 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 434 | Loss: 4.3650 | Acc: 42.83%\n",
      "Train - Epoch: 435 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 435 | Loss: 4.3752 | Acc: 43.26%\n",
      "Train - Epoch: 436 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 436 | Loss: 4.3766 | Acc: 43.26%\n",
      "Train - Epoch: 437 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 437 | Loss: 4.3853 | Acc: 43.26%\n",
      "Train - Epoch: 438 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 438 | Loss: 4.3947 | Acc: 43.26%\n",
      "Train - Epoch: 439 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 439 | Loss: 4.3995 | Acc: 43.04%\n",
      "Train - Epoch: 440 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 440 | Loss: 4.4053 | Acc: 42.61%\n",
      "Train - Epoch: 441 | Loss: 0.0000 | Acc: 100.00%\n",
      "Val - Epoch: 441 | Loss: 4.4182 | Acc: 43.26%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 312\u001b[0m\n\u001b[1;32m    303\u001b[0m trainer \u001b[38;5;241m=\u001b[39m EEGTrainer(\n\u001b[1;32m    304\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    305\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[1;32m    309\u001b[0m )\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 215\u001b[0m, in \u001b[0;36mEEGTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# Train and validate\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_epoch(epoch)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# Log metrics\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 160\u001b[0m, in \u001b[0;36mEEGTrainer.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    157\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import json\n",
    "\n",
    "class EEGTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader=None, config=None):\n",
    "        \"\"\"\n",
    "        Initialize the EEG trainer\n",
    "        \n",
    "        Args:\n",
    "            model: EEGAttentionNet model\n",
    "            train_loader: Training data loader\n",
    "            val_loader: Validation data loader\n",
    "            test_loader: Optional test data loader\n",
    "            config: Dictionary of training configuration\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        \n",
    "        # Set device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # Default configuration\n",
    "        self.config = {\n",
    "            'lr': 1e-3,\n",
    "            'weight_decay': 1e-4,\n",
    "            'patience': 10,\n",
    "            'min_lr': 1e-6,\n",
    "            'epochs': 100,\n",
    "            'save_dir': 'experiments',\n",
    "            'experiment_name': f'exp_{time.strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "            'save_attention_maps': True,\n",
    "            'attention_map_freq': 5\n",
    "        }\n",
    "        \n",
    "        # Update with user config if provided\n",
    "        if config:\n",
    "            self.config.update(config)\n",
    "            \n",
    "        # Create experiment directory\n",
    "        self.exp_dir = os.path.join(self.config['save_dir'], self.config['experiment_name'])\n",
    "        os.makedirs(self.exp_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize components\n",
    "        self._init_components()\n",
    "        \n",
    "        # Save config\n",
    "        self._save_config()\n",
    "        \n",
    "    def _init_components(self):\n",
    "        \"\"\"Initialize training components\"\"\"\n",
    "        # Loss function (CrossEntropy + KLDiv for attention regularization)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config['lr'],\n",
    "            weight_decay=self.config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=self.config['patience']//2,\n",
    "            min_lr=self.config['min_lr'],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Early stopping counter\n",
    "        self.best_val_acc = 0.0\n",
    "        self.early_stop_counter = 0\n",
    "        \n",
    "        # Tensorboard writer\n",
    "        self.writer = SummaryWriter(log_dir=self.exp_dir)\n",
    "        \n",
    "        # Attention maps directory\n",
    "        if self.config['save_attention_maps']:\n",
    "            self.attention_dir = os.path.join(self.exp_dir, 'attention_maps')\n",
    "            os.makedirs(self.attention_dir, exist_ok=True)\n",
    "    \n",
    "    def _save_config(self):\n",
    "        \"\"\"Save training configuration to JSON file\"\"\"\n",
    "        config_path = os.path.join(self.exp_dir, 'config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.config, f, indent=4)\n",
    "    \n",
    "    def _compute_metrics(self, outputs, labels):\n",
    "        \"\"\"Compute accuracy and other metrics\"\"\"\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy = correct / labels.size(0)\n",
    "        return accuracy\n",
    "    \n",
    "    def _log_metrics(self, phase, metrics, epoch):\n",
    "        \"\"\"Log metrics to Tensorboard and print to console\"\"\"\n",
    "        loss = metrics['loss']\n",
    "        acc = metrics['accuracy']\n",
    "        \n",
    "        # Console logging\n",
    "        print(f\"{phase.capitalize()} - Epoch: {epoch+1} | \"\n",
    "              f\"Loss: {loss:.4f} | Acc: {acc:.2%}\")\n",
    "        \n",
    "        # Tensorboard logging\n",
    "        self.writer.add_scalar(f'Loss/{phase}', loss, epoch)\n",
    "        self.writer.add_scalar(f'Accuracy/{phase}', acc, epoch)\n",
    "        \n",
    "        # Log learning rate\n",
    "        if phase == 'train':\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            self.writer.add_scalar('LR', lr, epoch)\n",
    "    \n",
    "    def _save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler': self.scheduler.state_dict(),\n",
    "            'best_val_acc': self.best_val_acc\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = os.path.join(self.exp_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "        torch.save(state, checkpoint_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            best_path = os.path.join(self.exp_dir, 'best_model.pth')\n",
    "            torch.save(state, best_path)\n",
    "    \n",
    "\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for inputs, labels in self.train_loader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            #print(labels)\n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Compute metrics\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_acc += self._compute_metrics(outputs, labels) * batch_size\n",
    "            total_samples += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_acc / total_samples\n",
    "        \n",
    "        return {\n",
    "            'loss': epoch_loss,\n",
    "            'accuracy': epoch_acc,\n",
    "        }\n",
    "    \n",
    "    def validate_epoch(self, epoch):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "     \n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                # Compute metrics\n",
    "                batch_size = inputs.size(0)\n",
    "                running_loss += loss.item() * batch_size\n",
    "                running_acc += self._compute_metrics(outputs, labels) * batch_size\n",
    "                total_samples += batch_size\n",
    "                \n",
    "        \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_acc / total_samples\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'loss': epoch_loss,\n",
    "            'accuracy': epoch_acc,\n",
    "        }\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.config['epochs']):\n",
    "            # Train and validate\n",
    "            train_metrics = self.train_epoch(epoch)\n",
    "            val_metrics = self.validate_epoch(epoch)\n",
    "            \n",
    "            # Log metrics\n",
    "            self._log_metrics('train', train_metrics, epoch)\n",
    "            self._log_metrics('val', val_metrics, epoch)\n",
    "            \n",
    "            # Save attention maps periodically\n",
    "            \n",
    "            # Step scheduler\n",
    "            self.scheduler.step(val_metrics['accuracy'])\n",
    "            \n",
    "            # Check for best model\n",
    "            if val_metrics['accuracy'] > self.best_val_acc:\n",
    "                self.best_val_acc = val_metrics['accuracy']\n",
    "                self._save_checkpoint(epoch, is_best=True)\n",
    "                self.early_stop_counter = 0\n",
    "            else:\n",
    "                self.early_stop_counter += 1\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if epoch % 10 == 0:\n",
    "                self._save_checkpoint(epoch)\n",
    "            \n",
    "            # Early stopping\n",
    "            # if self.early_stop_counter >= self.config['patience']:\n",
    "            #     print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            #     break\n",
    "        \n",
    "        # Training complete\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time//60:.0f}m {training_time%60:.0f}s\")\n",
    "        print(f\"Best validation accuracy: {self.best_val_acc:.2%}\")\n",
    "        \n",
    "        # Test if test loader provided\n",
    "        if self.test_loader:\n",
    "            test_acc = self.test()\n",
    "            print(f\"Test accuracy: {test_acc:.2%}\")\n",
    "        \n",
    "        # Close tensorboard writer\n",
    "        self.writer.close()\n",
    "        \n",
    "        return self.best_val_acc\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"Evaluate on test set\"\"\"\n",
    "        self.model.eval()\n",
    "        running_acc = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Load best model\n",
    "        best_path = os.path.join(self.exp_dir, 'best_model.pth')\n",
    "        if os.path.exists(best_path):\n",
    "            checkpoint = torch.load(best_path)\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                outputs, _ = self.model(inputs)\n",
    "                running_acc += self._compute_metrics(outputs, labels) * labels.size(0)\n",
    "                total_samples += labels.size(0)\n",
    "        \n",
    "        test_acc = running_acc / total_samples\n",
    "        self.writer.add_scalar('Accuracy/test', test_acc)\n",
    "        \n",
    "        return test_acc\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example configuration\n",
    "    config = {\n",
    "        'lr': 3e-4,\n",
    "        'weight_decay': 1e-5,\n",
    "        'patience': 15,\n",
    "        'epochs': 500,\n",
    "        'experiment_name': 'eeg_attention_experiment',\n",
    "        'save_attention_maps': True\n",
    "    }\n",
    "    \n",
    "    # Initialize components (using the previous data loader and model code)\n",
    "    model = EEGClassifier(num_classes=4)\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(data_dir=\"/teamspace/studios/this_studio/EEG_REC\", subjects = [1,2,3,5,6,7,8,9])\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = EEGTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    best_val_acc = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2304 individual epochs to /teamspace/studios/this_studio/epochs\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch data shape: (22, 31, 95)\n",
      "Epoch label: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the saved epoch\n",
    "epoch_file = '/teamspace/studios/this_studio/epochs/epoch_0.npy'\n",
    "loaded = np.load(epoch_file, allow_pickle=True).item()\n",
    "\n",
    "# Access data and label\n",
    "epoch_data = loaded['data']\n",
    "epoch_label = loaded['label']\n",
    "\n",
    "print(\"Epoch data shape:\", epoch_data.shape)\n",
    "print(\"Epoch label:\", epoch_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy input shape: torch.Size([16, 22, 31, 95])\n",
      "\n",
      "Model Architecture:\n",
      "EEGDecoder(\n",
      "  (channel_attention): ChannelAttention(\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=22, out_features=5, bias=False)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=5, out_features=22, bias=False)\n",
      "    )\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      "  (multihead_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=95, out_features=95, bias=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=64790, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Output logits shape: torch.Size([16, 4])\n",
      "Output shape is correct!\n",
      "Example probabilities for the first sample: tensor([0.2553, 0.2518, 0.2418, 0.2511], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, num_channels, reduction_ratio=4):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_channels, num_channels // reduction_ratio, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channels // reduction_ratio, num_channels, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid() # Sigmoid activation to scale weights between 0 and 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x).squeeze(-1).squeeze(-1))\n",
    "        max_out = self.fc(self.max_pool(x).squeeze(-1).squeeze(-1))\n",
    "\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "class EEGDecoder(nn.Module):\n",
    "    def __init__(self, num_channels=22, freq_bins=64, time_bins=76, num_classes=4,\n",
    "                 mha_num_heads=5, mha_dropout=0.1, channel_attention_reduction=4):\n",
    "        super(EEGDecoder, self).__init__()\n",
    "\n",
    "        self.num_channels = num_channels\n",
    "        self.freq_bins = freq_bins\n",
    "        self.time_bins = time_bins\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.channel_attention = ChannelAttention(num_channels, reduction_ratio=channel_attention_reduction)\n",
    "\n",
    "        self.multihead_attention = nn.MultiheadAttention(\n",
    "            embed_dim=time_bins,      # The embedding dimension for MHA, must match the last dimension of input\n",
    "            num_heads=mha_num_heads,  # Number of parallel attention heads\n",
    "            dropout=mha_dropout,      # Dropout rate for attention weights\n",
    "            batch_first=False         # Specifies input format: (seq_len, batch_size, embed_dim)\n",
    "        )\n",
    "\n",
    "        flattened_size = num_channels * freq_bins * time_bins\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                   # Flattens the input tensor (N, C, F, T) into (N, C*F*T)\n",
    "            nn.Linear(flattened_size, 512), # First fully connected layer with 512 hidden units\n",
    "            nn.ReLU(),                      # ReLU activation function\n",
    "            nn.Dropout(0.5),                # Dropout for regularization to prevent overfitting\n",
    "            nn.Linear(512, num_classes)     # Output layer, mapping to the number of classification classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, F, T = x.shape\n",
    "\n",
    "        channel_attn_weights = self.channel_attention(x) # Shape: (N, C, 1, 1)\n",
    "        x = x * channel_attn_weights                     # Resulting shape: (N, C, F, T)\n",
    "\n",
    "        x_reshaped_for_mha = x.view(N * C, F, T).permute(1, 0, 2)\n",
    "\n",
    "        mha_output, _ = self.multihead_attention(\n",
    "            query=x_reshaped_for_mha,\n",
    "            key=x_reshaped_for_mha,\n",
    "            value=x_reshaped_for_mha\n",
    "        ) # Output shape: (F, N*C, T)\n",
    "\n",
    "        x = mha_output.permute(1, 0, 2).view(N, C, F, T)\n",
    "\n",
    "        logits = self.classifier(x) # Output shape: (N, num_classes)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     num_channels = 22\n",
    "#     freq_bins = 31\n",
    "#     time_bins = 95\n",
    "#     num_classes = 4\n",
    "#     batch_size = 16 # Example batch size\n",
    "\n",
    "#     dummy_input = torch.randn(batch_size, num_channels, freq_bins, time_bins)\n",
    "#     print(f\"Dummy input shape: {dummy_input.shape}\")\n",
    "\n",
    "#     model = EEGDecoder(\n",
    "#         num_channels=num_channels,\n",
    "#         freq_bins=freq_bins,\n",
    "#         time_bins=time_bins,\n",
    "#         num_classes=num_classes\n",
    "#     )\n",
    "#     print(\"\\nModel Architecture:\")\n",
    "#     print(model)\n",
    "\n",
    "#     output_logits = model(dummy_input)\n",
    "#     print(f\"\\nOutput logits shape: {output_logits.shape}\")\n",
    "\n",
    "#     expected_output_shape = (batch_size, num_classes)\n",
    "#     assert output_logits.shape == expected_output_shape, \\\n",
    "#         f\"Output shape mismatch! Expected {expected_output_shape}, got {output_logits.shape}\"\n",
    "#     print(\"Output shape is correct!\")\n",
    "\n",
    "#     probabilities = F.softmax(output_logits, dim=1)\n",
    "#     print(f\"Example probabilities for the first sample: {probabilities[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
