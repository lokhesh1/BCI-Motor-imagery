{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from scipy.signal import firwin, filtfilt\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"mne\")\n",
    "# Completely silence MNE-Python output\n",
    "mne.set_log_level('WARNING')  # or 'ERROR' for even less output\n",
    "logging.getLogger('mne').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eeg(path):\n",
    "    raw = mne.io.read_raw_gdf(path, preload=True)\n",
    "\n",
    "    # Step 2: Get the sampling frequency\n",
    "    sfreq = raw.info['sfreq']  # Hz\n",
    "\n",
    "    # Step 3: Define FIR filter parameters\n",
    "    low_cutoff = 1.0   # Hz\n",
    "    high_cutoff = 30.0 # Hz\n",
    "    filter_order = 177 # Must be odd for linear-phase FIR\n",
    "\n",
    "    nyquist = 0.5 * sfreq\n",
    "\n",
    "\n",
    "    fir_coeffs = firwin(\n",
    "        numtaps=filter_order,\n",
    "        cutoff=[low_cutoff / nyquist, high_cutoff / nyquist],\n",
    "        pass_zero=False,\n",
    "        window='blackman'\n",
    "    )\n",
    "    eeg_data = raw.get_data()\n",
    "    filtered_data = filtfilt(fir_coeffs, 1.0, eeg_data, axis=1)\n",
    "\n",
    "    new_raw = mne.io.RawArray(filtered_data, raw.info.copy())\n",
    "    annotations = raw.annotations \n",
    "    new_raw.set_annotations(annotations)\n",
    "\n",
    "    return new_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path, test=False):\n",
    "    raw = load_eeg(path)\n",
    "    eeg_channels = raw.ch_names[:22]\n",
    "    raw.pick(eeg_channels)\n",
    "    \n",
    "    if not test:\n",
    "        events, events_id = mne.events_from_annotations(raw, event_id= {'769': 0,'770': 1,'771': 2,'772': 3})\n",
    "    else:\n",
    "        events, events_id = mne.events_from_annotations(raw, event_id= {'768':6})\n",
    "        #print(events_id)\n",
    "    #print(events_id)\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        events=events,  \n",
    "        tmin=0,     \n",
    "        tmax=4.0,\n",
    "        event_id=events_id,\n",
    "        baseline=None,\n",
    "        preload=True\n",
    "    )\n",
    "\n",
    "    labels = epochs.events[:, 2]\n",
    "    #print(labels)\n",
    "    data = epochs.get_data()\n",
    "    \n",
    "    return {\n",
    "        'epochs': data,   \n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "a = preprocess('/teamspace/studios/this_studio/EEG_REC/A01T.gdf')\n",
    "# print(a['epochs'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#start of trials (768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BCI4_2a_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, subjects=[1], train=True, transform=None, target_transform=None, test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.subjects = subjects\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.test=test\n",
    "        self.data, self.labels = self._load_data()\n",
    "        \n",
    "        self.labels = self.labels \n",
    "        \n",
    "    def _load_data(self):\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for subject in self.subjects:\n",
    "            if self.train:\n",
    "                filename = f'A{subject:02d}T.gdf'\n",
    "            else:\n",
    "                filename = f'A{subject:02d}E.gdf'\n",
    "                \n",
    "            filepath = os.path.join(self.data_dir, filename)\n",
    "            data_ = preprocess(filepath, self.test)\n",
    "            \n",
    "            data = data_['epochs']\n",
    "            labels = data_['labels']\n",
    "           \n",
    "            data = np.transpose(data, (0, 2, 1))\n",
    "            \n",
    "            all_data.append(data)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "        all_data = np.concatenate(all_data, axis=0)\n",
    "        all_labels = np.concatenate(all_labels, axis=0)\n",
    "        #print(all_labels.shape)\n",
    "        def normalize_eeg(trial_data):\n",
    "            \"\"\"trial_data shape: (channels, timepoints)\"\"\"\n",
    "            means = trial_data.mean(axis=1, keepdims=True)\n",
    "            stds = trial_data.std(axis=1, keepdims=True)\n",
    "            return (trial_data - means) / (stds + 1e-8)\n",
    "\n",
    "        all_data_n = normalize_eeg(all_data)\n",
    "\n",
    "        return all_data_n, all_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        x = torch.from_numpy(x).float().unsqueeze(0)  # shape: (1, timepoints, channels)\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "def get_data_loaders(data_dir, subjects=[1], batch_size=32, val_split=0.2, test_split=0.2, random_state=42):\n",
    "\n",
    "    full_train_dataset = BCI4_2a_Dataset(data_dir, subjects=subjects, train=True)\n",
    "    \n",
    "    test_dataset = BCI4_2a_Dataset(data_dir, subjects=subjects, train=False, test=True )\n",
    "    \n",
    "    train_idx, val_idx = train_test_split(\n",
    "        range(len(full_train_dataset)),\n",
    "        test_size=val_split,\n",
    "        random_state=random_state,\n",
    "        stratify=full_train_dataset.labels\n",
    "    )\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(full_train_dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(full_train_dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     DATA_DIR = \"/teamspace/studios/this_studio/EEG_REC\"\n",
    "    \n",
    "#     train_loader, val_loader, test_loader = get_data_loaders(\n",
    "#         data_dir=DATA_DIR,\n",
    "#         subjects=[1,2,3,5,6,7,8,9],\n",
    "#         batch_size=32\n",
    "#     )\n",
    "    \n",
    "#     print(\"Dataset sizes:\")\n",
    "#     print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "#     print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "#     print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "    \n",
    "#     x, y = next(iter(train_loader))\n",
    "#     print(\"\\nBatch shape:\")\n",
    "#     print(f\"Input shape: {x.shape}\")  # Should be (batch_size, 1, timepoints, channels)\n",
    "#     print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel-wise attention mechanism for EEG signals\"\"\"\n",
    "    def __init__(self, num_channels, reduction_ratio=4):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_channels, num_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channels // reduction_ratio, num_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        channel_avg = torch.mean(x, dim=2, keepdim=True)  # (batch_size, 1, 1, channels)\n",
    "        channel_max, _ = torch.max(x, dim=2, keepdim=True) # (batch_size, 1, 1, channels)\n",
    "        \n",
    "        combined = channel_avg + channel_max  # (batch_size, 1, 1, channels)\n",
    "        combined = combined.squeeze(1).squeeze(1)  # (batch_size, channels)\n",
    "        \n",
    "        attention = self.mlp(combined)  # (batch_size, channels)\n",
    "        attention = torch.sigmoid(attention).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, channels)\n",
    "        \n",
    "        attended_x = x * attention\n",
    "        \n",
    "        return attended_x, attention.squeeze()\n",
    "\n",
    "class TemporalFeatureExtractor(nn.Module):\n",
    "    \"\"\"Temporal feature extractor with depthwise separable convolutions\"\"\"\n",
    "    def __init__(self, num_channels, timepoints):\n",
    "        super(TemporalFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Depthwise separable convolutions with BatchNorm\n",
    "        self.depthwise = nn.Sequential(\n",
    "            nn.Conv2d(1, num_channels, kernel_size=(1, 64), \n",
    "            groups=1, padding=(0, 32)),\n",
    "            nn.BatchNorm2d(num_channels),\n",
    "            nn.ELU()  # Using ELU for EEG data often works better\n",
    "        )\n",
    "        \n",
    "        self.pointwise = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 32, kernel_size=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        # Temporal convolution with BatchNorm\n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(1, 16), \n",
    "                     stride=(1, 4), padding=(0, 8)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.3)  # Added dropout for better regularization\n",
    "        )\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 64))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.depthwise(x))\n",
    "        x = F.relu(self.pointwise(x))\n",
    "        \n",
    "        x = F.relu(self.temporal_conv(x))\n",
    "        \n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class EEGAttentionNet(nn.Module):\n",
    "    def __init__(self, num_channels=22, num_classes=4, timepoints=1125):\n",
    "        super(EEGAttentionNet, self).__init__()\n",
    "        \n",
    "        self.channel_attention = ChannelAttention(num_channels)\n",
    "        self.temporal_extractor = TemporalFeatureExtractor(num_channels, timepoints)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 64, 128),\n",
    "            nn.BatchNorm1d(128),  # Added BatchNorm\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        attended_x, attention_weights = self.channel_attention(x)\n",
    "        \n",
    "        temporal_features = self.temporal_extractor(attended_x)\n",
    "        \n",
    "        logits = self.classifier(temporal_features)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     NUM_CHANNELS = 22  # BCI-IV 2a has 22 EEG channels\n",
    "#     NUM_CLASSES = 4    # 4 motor imagery tasks\n",
    "#     TIMEPOINTS = 1001  # Typical sample length in the dataset\n",
    "    \n",
    "#     # Create model\n",
    "#     model = EEGAttentionNet(num_channels=NUM_CHANNELS, \n",
    "#                           num_classes=NUM_CLASSES,\n",
    "#                           timepoints=TIMEPOINTS)\n",
    "    \n",
    "#     print(model)\n",
    "    \n",
    "#     batch_size = 32\n",
    "#     x = torch.randn(batch_size, 1, TIMEPOINTS, NUM_CHANNELS)\n",
    "#     logits, attention = model(x)\n",
    "    \n",
    "#     print(f\"\\nInput shape: {x.shape}\")\n",
    "#     print(f\"Output logits shape: {logits.shape}\")\n",
    "#     print(f\"Attention weights shape: {attention.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 08:49:45.235074: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-21 08:49:45.281961: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-21 08:49:45.282009: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-21 08:49:45.282043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-21 08:49:45.291023: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-21 08:49:46.410725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Epoch: 1 | Loss: 1.4420 | Acc: 31.31%\n",
      "Val - Epoch: 1 | Loss: 1.3227 | Acc: 34.06%\n",
      "Train - Epoch: 2 | Loss: 1.3500 | Acc: 36.68%\n",
      "Val - Epoch: 2 | Loss: 1.4187 | Acc: 29.50%\n",
      "Train - Epoch: 3 | Loss: 1.3153 | Acc: 39.23%\n",
      "Val - Epoch: 3 | Loss: 1.4910 | Acc: 38.61%\n",
      "Train - Epoch: 4 | Loss: 1.2824 | Acc: 39.45%\n",
      "Val - Epoch: 4 | Loss: 1.4825 | Acc: 27.98%\n",
      "Train - Epoch: 5 | Loss: 1.2484 | Acc: 42.54%\n",
      "Val - Epoch: 5 | Loss: 1.3176 | Acc: 40.35%\n",
      "Train - Epoch: 6 | Loss: 1.2119 | Acc: 47.26%\n",
      "Val - Epoch: 6 | Loss: 1.7936 | Acc: 30.80%\n",
      "Train - Epoch: 7 | Loss: 1.1891 | Acc: 46.55%\n",
      "Val - Epoch: 7 | Loss: 1.3239 | Acc: 40.56%\n",
      "Train - Epoch: 8 | Loss: 1.1532 | Acc: 50.30%\n",
      "Val - Epoch: 8 | Loss: 1.5068 | Acc: 34.49%\n",
      "Train - Epoch: 9 | Loss: 1.1147 | Acc: 50.57%\n",
      "Val - Epoch: 9 | Loss: 1.5174 | Acc: 40.35%\n",
      "Train - Epoch: 10 | Loss: 1.0927 | Acc: 52.58%\n",
      "Val - Epoch: 10 | Loss: 1.2459 | Acc: 44.03%\n",
      "Train - Epoch: 11 | Loss: 1.0714 | Acc: 53.34%\n",
      "Val - Epoch: 11 | Loss: 1.2674 | Acc: 40.35%\n",
      "Train - Epoch: 12 | Loss: 1.0411 | Acc: 55.02%\n",
      "Val - Epoch: 12 | Loss: 1.3208 | Acc: 45.12%\n",
      "Train - Epoch: 13 | Loss: 1.0307 | Acc: 56.48%\n",
      "Val - Epoch: 13 | Loss: 1.5680 | Acc: 38.18%\n",
      "Train - Epoch: 14 | Loss: 0.9970 | Acc: 57.95%\n",
      "Val - Epoch: 14 | Loss: 1.3095 | Acc: 42.73%\n",
      "Train - Epoch: 15 | Loss: 0.9774 | Acc: 58.65%\n",
      "Val - Epoch: 15 | Loss: 1.5441 | Acc: 36.66%\n",
      "Train - Epoch: 16 | Loss: 0.9542 | Acc: 58.38%\n",
      "Val - Epoch: 16 | Loss: 1.6586 | Acc: 43.60%\n",
      "Train - Epoch: 17 | Loss: 0.9378 | Acc: 60.72%\n",
      "Val - Epoch: 17 | Loss: 1.7352 | Acc: 39.26%\n",
      "Train - Epoch: 18 | Loss: 0.9259 | Acc: 60.88%\n",
      "Val - Epoch: 18 | Loss: 1.4602 | Acc: 45.12%\n",
      "Train - Epoch: 19 | Loss: 0.8900 | Acc: 62.94%\n",
      "Val - Epoch: 19 | Loss: 1.6907 | Acc: 37.96%\n",
      "Train - Epoch: 20 | Loss: 0.8608 | Acc: 64.51%\n",
      "Val - Epoch: 20 | Loss: 1.7031 | Acc: 36.23%\n",
      "Train - Epoch: 21 | Loss: 0.7928 | Acc: 69.29%\n",
      "Val - Epoch: 21 | Loss: 1.4882 | Acc: 44.69%\n",
      "Train - Epoch: 22 | Loss: 0.7546 | Acc: 69.78%\n",
      "Val - Epoch: 22 | Loss: 1.3830 | Acc: 42.73%\n",
      "Train - Epoch: 23 | Loss: 0.7205 | Acc: 70.65%\n",
      "Val - Epoch: 23 | Loss: 1.3153 | Acc: 50.54%\n",
      "Train - Epoch: 24 | Loss: 0.7384 | Acc: 71.03%\n",
      "Val - Epoch: 24 | Loss: 1.5606 | Acc: 42.73%\n",
      "Train - Epoch: 25 | Loss: 0.6853 | Acc: 72.98%\n",
      "Val - Epoch: 25 | Loss: 1.3053 | Acc: 48.37%\n",
      "Train - Epoch: 26 | Loss: 0.6696 | Acc: 73.30%\n",
      "Val - Epoch: 26 | Loss: 1.9291 | Acc: 34.92%\n",
      "Train - Epoch: 27 | Loss: 0.6756 | Acc: 74.01%\n",
      "Val - Epoch: 27 | Loss: 1.7305 | Acc: 44.69%\n",
      "Train - Epoch: 28 | Loss: 0.6677 | Acc: 73.58%\n",
      "Val - Epoch: 28 | Loss: 1.6581 | Acc: 44.69%\n",
      "Train - Epoch: 29 | Loss: 0.6464 | Acc: 75.04%\n",
      "Val - Epoch: 29 | Loss: 1.4330 | Acc: 46.85%\n",
      "Train - Epoch: 30 | Loss: 0.6467 | Acc: 74.44%\n",
      "Val - Epoch: 30 | Loss: 1.5372 | Acc: 42.52%\n",
      "Train - Epoch: 31 | Loss: 0.5962 | Acc: 76.72%\n",
      "Val - Epoch: 31 | Loss: 1.4061 | Acc: 46.42%\n",
      "Train - Epoch: 32 | Loss: 0.5506 | Acc: 80.09%\n",
      "Val - Epoch: 32 | Loss: 1.6127 | Acc: 46.64%\n",
      "Train - Epoch: 33 | Loss: 0.5339 | Acc: 79.92%\n",
      "Val - Epoch: 33 | Loss: 1.4789 | Acc: 46.42%\n",
      "Train - Epoch: 34 | Loss: 0.5221 | Acc: 80.47%\n",
      "Val - Epoch: 34 | Loss: 1.3639 | Acc: 48.16%\n",
      "Train - Epoch: 35 | Loss: 0.5287 | Acc: 81.06%\n",
      "Val - Epoch: 35 | Loss: 1.4059 | Acc: 45.55%\n",
      "Train - Epoch: 36 | Loss: 0.5071 | Acc: 81.82%\n",
      "Val - Epoch: 36 | Loss: 1.3852 | Acc: 46.64%\n",
      "Train - Epoch: 37 | Loss: 0.5100 | Acc: 81.17%\n",
      "Val - Epoch: 37 | Loss: 1.5240 | Acc: 46.20%\n",
      "Train - Epoch: 38 | Loss: 0.4781 | Acc: 82.58%\n",
      "Val - Epoch: 38 | Loss: 1.4238 | Acc: 47.07%\n",
      "Train - Epoch: 39 | Loss: 0.4706 | Acc: 82.42%\n",
      "Val - Epoch: 39 | Loss: 1.5035 | Acc: 45.34%\n",
      "Train - Epoch: 40 | Loss: 0.4602 | Acc: 83.23%\n",
      "Val - Epoch: 40 | Loss: 1.4065 | Acc: 50.11%\n",
      "Train - Epoch: 41 | Loss: 0.4429 | Acc: 84.16%\n",
      "Val - Epoch: 41 | Loss: 1.4338 | Acc: 47.51%\n",
      "Train - Epoch: 42 | Loss: 0.4401 | Acc: 84.16%\n",
      "Val - Epoch: 42 | Loss: 1.4502 | Acc: 47.29%\n",
      "Train - Epoch: 43 | Loss: 0.4279 | Acc: 83.61%\n",
      "Val - Epoch: 43 | Loss: 1.4657 | Acc: 47.72%\n",
      "Train - Epoch: 44 | Loss: 0.4239 | Acc: 84.75%\n",
      "Val - Epoch: 44 | Loss: 1.5284 | Acc: 46.64%\n",
      "Train - Epoch: 45 | Loss: 0.4253 | Acc: 84.59%\n",
      "Val - Epoch: 45 | Loss: 1.4501 | Acc: 48.59%\n",
      "Train - Epoch: 46 | Loss: 0.4121 | Acc: 85.19%\n",
      "Val - Epoch: 46 | Loss: 1.5026 | Acc: 46.20%\n",
      "Train - Epoch: 47 | Loss: 0.3995 | Acc: 85.35%\n",
      "Val - Epoch: 47 | Loss: 1.4681 | Acc: 48.81%\n",
      "Train - Epoch: 48 | Loss: 0.3831 | Acc: 86.49%\n",
      "Val - Epoch: 48 | Loss: 1.4854 | Acc: 47.51%\n",
      "Train - Epoch: 49 | Loss: 0.3857 | Acc: 86.11%\n",
      "Val - Epoch: 49 | Loss: 1.4816 | Acc: 47.94%\n",
      "Train - Epoch: 50 | Loss: 0.3733 | Acc: 86.81%\n",
      "Val - Epoch: 50 | Loss: 1.4842 | Acc: 48.81%\n",
      "Train - Epoch: 51 | Loss: 0.4071 | Acc: 85.35%\n",
      "Val - Epoch: 51 | Loss: 1.4680 | Acc: 47.51%\n",
      "Train - Epoch: 52 | Loss: 0.3899 | Acc: 85.46%\n",
      "Val - Epoch: 52 | Loss: 1.4782 | Acc: 47.51%\n",
      "Train - Epoch: 53 | Loss: 0.3957 | Acc: 85.40%\n",
      "Val - Epoch: 53 | Loss: 1.4846 | Acc: 48.37%\n",
      "Train - Epoch: 54 | Loss: 0.3760 | Acc: 87.19%\n",
      "Val - Epoch: 54 | Loss: 1.4903 | Acc: 48.37%\n",
      "Train - Epoch: 55 | Loss: 0.3763 | Acc: 86.27%\n",
      "Val - Epoch: 55 | Loss: 1.4994 | Acc: 47.51%\n",
      "Train - Epoch: 56 | Loss: 0.3782 | Acc: 86.71%\n",
      "Val - Epoch: 56 | Loss: 1.4936 | Acc: 48.16%\n",
      "Train - Epoch: 57 | Loss: 0.3613 | Acc: 87.36%\n",
      "Val - Epoch: 57 | Loss: 1.4933 | Acc: 47.51%\n",
      "Train - Epoch: 58 | Loss: 0.3610 | Acc: 87.03%\n",
      "Val - Epoch: 58 | Loss: 1.4853 | Acc: 48.16%\n",
      "Train - Epoch: 59 | Loss: 0.3539 | Acc: 88.50%\n",
      "Val - Epoch: 59 | Loss: 1.5075 | Acc: 47.72%\n",
      "Train - Epoch: 60 | Loss: 0.3583 | Acc: 88.17%\n",
      "Val - Epoch: 60 | Loss: 1.5073 | Acc: 47.94%\n",
      "Train - Epoch: 61 | Loss: 0.3618 | Acc: 87.74%\n",
      "Val - Epoch: 61 | Loss: 1.5152 | Acc: 47.29%\n",
      "Train - Epoch: 62 | Loss: 0.3776 | Acc: 85.78%\n",
      "Val - Epoch: 62 | Loss: 1.5230 | Acc: 47.94%\n",
      "Train - Epoch: 63 | Loss: 0.3737 | Acc: 86.49%\n",
      "Val - Epoch: 63 | Loss: 1.5081 | Acc: 47.51%\n",
      "Train - Epoch: 64 | Loss: 0.3596 | Acc: 87.25%\n",
      "Val - Epoch: 64 | Loss: 1.5048 | Acc: 47.72%\n",
      "Train - Epoch: 65 | Loss: 0.3436 | Acc: 88.12%\n",
      "Val - Epoch: 65 | Loss: 1.5123 | Acc: 47.72%\n",
      "Train - Epoch: 66 | Loss: 0.3503 | Acc: 88.33%\n",
      "Val - Epoch: 66 | Loss: 1.5051 | Acc: 47.07%\n",
      "Train - Epoch: 67 | Loss: 0.3485 | Acc: 87.90%\n",
      "Val - Epoch: 67 | Loss: 1.5170 | Acc: 47.72%\n",
      "Train - Epoch: 68 | Loss: 0.3435 | Acc: 87.47%\n",
      "Val - Epoch: 68 | Loss: 1.5292 | Acc: 47.51%\n",
      "Train - Epoch: 69 | Loss: 0.3535 | Acc: 87.57%\n",
      "Val - Epoch: 69 | Loss: 1.5214 | Acc: 48.16%\n",
      "Train - Epoch: 70 | Loss: 0.3664 | Acc: 87.47%\n",
      "Val - Epoch: 70 | Loss: 1.5226 | Acc: 47.07%\n",
      "Train - Epoch: 71 | Loss: 0.3604 | Acc: 87.68%\n",
      "Val - Epoch: 71 | Loss: 1.5309 | Acc: 47.29%\n",
      "Train - Epoch: 72 | Loss: 0.3528 | Acc: 87.36%\n",
      "Val - Epoch: 72 | Loss: 1.5079 | Acc: 47.94%\n",
      "Train - Epoch: 73 | Loss: 0.3655 | Acc: 86.54%\n",
      "Val - Epoch: 73 | Loss: 1.4996 | Acc: 47.29%\n",
      "Train - Epoch: 74 | Loss: 0.3455 | Acc: 87.57%\n",
      "Val - Epoch: 74 | Loss: 1.5180 | Acc: 47.72%\n",
      "Train - Epoch: 75 | Loss: 0.3522 | Acc: 88.50%\n",
      "Val - Epoch: 75 | Loss: 1.5178 | Acc: 48.16%\n",
      "Train - Epoch: 76 | Loss: 0.3524 | Acc: 88.44%\n",
      "Val - Epoch: 76 | Loss: 1.5168 | Acc: 47.29%\n",
      "Train - Epoch: 77 | Loss: 0.3439 | Acc: 88.39%\n",
      "Val - Epoch: 77 | Loss: 1.5089 | Acc: 47.07%\n",
      "Train - Epoch: 78 | Loss: 0.3663 | Acc: 86.92%\n",
      "Val - Epoch: 78 | Loss: 1.5233 | Acc: 48.37%\n",
      "Train - Epoch: 79 | Loss: 0.3637 | Acc: 87.25%\n",
      "Val - Epoch: 79 | Loss: 1.5294 | Acc: 47.94%\n",
      "Train - Epoch: 80 | Loss: 0.3449 | Acc: 87.63%\n",
      "Val - Epoch: 80 | Loss: 1.5157 | Acc: 48.16%\n",
      "Train - Epoch: 81 | Loss: 0.3708 | Acc: 87.68%\n",
      "Val - Epoch: 81 | Loss: 1.5142 | Acc: 47.94%\n",
      "Train - Epoch: 82 | Loss: 0.3534 | Acc: 87.25%\n",
      "Val - Epoch: 82 | Loss: 1.5320 | Acc: 47.94%\n",
      "Train - Epoch: 83 | Loss: 0.3413 | Acc: 88.01%\n",
      "Val - Epoch: 83 | Loss: 1.5169 | Acc: 47.94%\n",
      "Train - Epoch: 84 | Loss: 0.3524 | Acc: 86.87%\n",
      "Val - Epoch: 84 | Loss: 1.5219 | Acc: 48.16%\n",
      "Train - Epoch: 85 | Loss: 0.3512 | Acc: 88.12%\n",
      "Val - Epoch: 85 | Loss: 1.5198 | Acc: 47.94%\n",
      "Train - Epoch: 86 | Loss: 0.3602 | Acc: 87.63%\n",
      "Val - Epoch: 86 | Loss: 1.5151 | Acc: 47.72%\n",
      "Train - Epoch: 87 | Loss: 0.3509 | Acc: 87.47%\n",
      "Val - Epoch: 87 | Loss: 1.5264 | Acc: 47.94%\n",
      "Train - Epoch: 88 | Loss: 0.3455 | Acc: 88.12%\n",
      "Val - Epoch: 88 | Loss: 1.5419 | Acc: 48.16%\n",
      "Train - Epoch: 89 | Loss: 0.3636 | Acc: 87.57%\n",
      "Val - Epoch: 89 | Loss: 1.5233 | Acc: 47.51%\n",
      "Train - Epoch: 90 | Loss: 0.3544 | Acc: 87.52%\n",
      "Val - Epoch: 90 | Loss: 1.5199 | Acc: 48.37%\n",
      "Train - Epoch: 91 | Loss: 0.3507 | Acc: 88.12%\n",
      "Val - Epoch: 91 | Loss: 1.5267 | Acc: 46.85%\n",
      "Train - Epoch: 92 | Loss: 0.3497 | Acc: 88.23%\n",
      "Val - Epoch: 92 | Loss: 1.5196 | Acc: 47.72%\n",
      "Train - Epoch: 93 | Loss: 0.3624 | Acc: 87.19%\n",
      "Val - Epoch: 93 | Loss: 1.5460 | Acc: 47.51%\n",
      "Train - Epoch: 94 | Loss: 0.3573 | Acc: 87.85%\n",
      "Val - Epoch: 94 | Loss: 1.5204 | Acc: 47.72%\n",
      "Train - Epoch: 95 | Loss: 0.3394 | Acc: 89.20%\n",
      "Val - Epoch: 95 | Loss: 1.5161 | Acc: 47.51%\n",
      "Train - Epoch: 96 | Loss: 0.3531 | Acc: 88.06%\n",
      "Val - Epoch: 96 | Loss: 1.5063 | Acc: 47.29%\n",
      "Train - Epoch: 97 | Loss: 0.3491 | Acc: 88.33%\n",
      "Val - Epoch: 97 | Loss: 1.5195 | Acc: 47.72%\n",
      "Train - Epoch: 98 | Loss: 0.3569 | Acc: 88.06%\n",
      "Val - Epoch: 98 | Loss: 1.5079 | Acc: 47.72%\n",
      "Train - Epoch: 99 | Loss: 0.3615 | Acc: 86.92%\n",
      "Val - Epoch: 99 | Loss: 1.5213 | Acc: 47.29%\n",
      "Train - Epoch: 100 | Loss: 0.3560 | Acc: 87.68%\n",
      "Val - Epoch: 100 | Loss: 1.5339 | Acc: 47.72%\n",
      "Train - Epoch: 101 | Loss: 0.3594 | Acc: 86.54%\n",
      "Val - Epoch: 101 | Loss: 1.5157 | Acc: 47.07%\n",
      "Train - Epoch: 102 | Loss: 0.3494 | Acc: 88.12%\n",
      "Val - Epoch: 102 | Loss: 1.5104 | Acc: 47.51%\n",
      "Train - Epoch: 103 | Loss: 0.3499 | Acc: 87.90%\n",
      "Val - Epoch: 103 | Loss: 1.5092 | Acc: 47.51%\n",
      "Train - Epoch: 104 | Loss: 0.3504 | Acc: 87.74%\n",
      "Val - Epoch: 104 | Loss: 1.5281 | Acc: 47.94%\n",
      "Train - Epoch: 105 | Loss: 0.3504 | Acc: 87.25%\n",
      "Val - Epoch: 105 | Loss: 1.5286 | Acc: 47.51%\n",
      "Train - Epoch: 106 | Loss: 0.3484 | Acc: 87.30%\n",
      "Val - Epoch: 106 | Loss: 1.5268 | Acc: 47.07%\n",
      "Train - Epoch: 107 | Loss: 0.3609 | Acc: 87.41%\n",
      "Val - Epoch: 107 | Loss: 1.5298 | Acc: 47.72%\n",
      "Train - Epoch: 108 | Loss: 0.3355 | Acc: 88.06%\n",
      "Val - Epoch: 108 | Loss: 1.5276 | Acc: 47.51%\n",
      "Train - Epoch: 109 | Loss: 0.3409 | Acc: 87.79%\n",
      "Val - Epoch: 109 | Loss: 1.5335 | Acc: 48.37%\n",
      "Train - Epoch: 110 | Loss: 0.3501 | Acc: 88.33%\n",
      "Val - Epoch: 110 | Loss: 1.5352 | Acc: 47.72%\n",
      "Train - Epoch: 111 | Loss: 0.3480 | Acc: 87.90%\n",
      "Val - Epoch: 111 | Loss: 1.5309 | Acc: 47.72%\n",
      "Train - Epoch: 112 | Loss: 0.3312 | Acc: 88.44%\n",
      "Val - Epoch: 112 | Loss: 1.5297 | Acc: 48.16%\n",
      "Train - Epoch: 113 | Loss: 0.3482 | Acc: 88.99%\n",
      "Val - Epoch: 113 | Loss: 1.5333 | Acc: 47.72%\n",
      "Train - Epoch: 114 | Loss: 0.3326 | Acc: 88.39%\n",
      "Val - Epoch: 114 | Loss: 1.5299 | Acc: 47.72%\n",
      "Train - Epoch: 115 | Loss: 0.3455 | Acc: 87.68%\n",
      "Val - Epoch: 115 | Loss: 1.5460 | Acc: 47.29%\n",
      "Train - Epoch: 116 | Loss: 0.3398 | Acc: 87.95%\n",
      "Val - Epoch: 116 | Loss: 1.5284 | Acc: 47.94%\n",
      "Train - Epoch: 117 | Loss: 0.3405 | Acc: 88.44%\n",
      "Val - Epoch: 117 | Loss: 1.5296 | Acc: 47.72%\n",
      "Train - Epoch: 118 | Loss: 0.3412 | Acc: 88.77%\n",
      "Val - Epoch: 118 | Loss: 1.5220 | Acc: 48.16%\n",
      "Train - Epoch: 119 | Loss: 0.3554 | Acc: 87.68%\n",
      "Val - Epoch: 119 | Loss: 1.5421 | Acc: 47.72%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 327\u001b[0m\n\u001b[1;32m    318\u001b[0m trainer \u001b[38;5;241m=\u001b[39m EEGTrainer(\n\u001b[1;32m    319\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    320\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig\n\u001b[1;32m    324\u001b[0m )\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 227\u001b[0m, in \u001b[0;36mEEGTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# Train and validate\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_epoch(epoch)\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# Log metrics\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 171\u001b[0m, in \u001b[0;36mEEGTrainer.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n\u001b[1;32m    170\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[1;32m    172\u001b[0m running_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_metrics(outputs, labels) \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[1;32m    173\u001b[0m total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import json\n",
    "\n",
    "class EEGTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader=None, config=None):\n",
    "        \"\"\"\n",
    "        Initialize the EEG trainer\n",
    "        \n",
    "        Args:\n",
    "            model: EEGAttentionNet model\n",
    "            train_loader: Training data loader\n",
    "            val_loader: Validation data loader\n",
    "            test_loader: Optional test data loader\n",
    "            config: Dictionary of training configuration\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        \n",
    "        # Set device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # Default configuration\n",
    "        self.config = {\n",
    "            'lr': 1e-3,\n",
    "            'weight_decay': 1e-4,\n",
    "            'patience': 10,\n",
    "            'min_lr': 1e-6,\n",
    "            'epochs': 100,\n",
    "            'save_dir': 'experiments',\n",
    "            'experiment_name': f'exp_{time.strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "            'save_attention_maps': True,\n",
    "            'attention_map_freq': 5\n",
    "        }\n",
    "        \n",
    "        # Update with user config if provided\n",
    "        if config:\n",
    "            self.config.update(config)\n",
    "            \n",
    "        # Create experiment directory\n",
    "        self.exp_dir = os.path.join(self.config['save_dir'], self.config['experiment_name'])\n",
    "        os.makedirs(self.exp_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize components\n",
    "        self._init_components()\n",
    "        \n",
    "        # Save config\n",
    "        self._save_config()\n",
    "        \n",
    "    def _init_components(self):\n",
    "        \"\"\"Initialize training components\"\"\"\n",
    "        # Loss function (CrossEntropy + KLDiv for attention regularization)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config['lr'],\n",
    "            weight_decay=self.config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=self.config['patience']//2,\n",
    "            min_lr=self.config['min_lr'],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Early stopping counter\n",
    "        self.best_val_acc = 0.0\n",
    "        self.early_stop_counter = 0\n",
    "        \n",
    "        # Tensorboard writer\n",
    "        self.writer = SummaryWriter(log_dir=self.exp_dir)\n",
    "        \n",
    "        # Attention maps directory\n",
    "        if self.config['save_attention_maps']:\n",
    "            self.attention_dir = os.path.join(self.exp_dir, 'attention_maps')\n",
    "            os.makedirs(self.attention_dir, exist_ok=True)\n",
    "    \n",
    "    def _save_config(self):\n",
    "        \"\"\"Save training configuration to JSON file\"\"\"\n",
    "        config_path = os.path.join(self.exp_dir, 'config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.config, f, indent=4)\n",
    "    \n",
    "    def _compute_metrics(self, outputs, labels):\n",
    "        \"\"\"Compute accuracy and other metrics\"\"\"\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy = correct / labels.size(0)\n",
    "        return accuracy\n",
    "    \n",
    "    def _log_metrics(self, phase, metrics, epoch):\n",
    "        \"\"\"Log metrics to Tensorboard and print to console\"\"\"\n",
    "        loss = metrics['loss']\n",
    "        acc = metrics['accuracy']\n",
    "        \n",
    "        # Console logging\n",
    "        print(f\"{phase.capitalize()} - Epoch: {epoch+1} | \"\n",
    "              f\"Loss: {loss:.4f} | Acc: {acc:.2%}\")\n",
    "        \n",
    "        # Tensorboard logging\n",
    "        self.writer.add_scalar(f'Loss/{phase}', loss, epoch)\n",
    "        self.writer.add_scalar(f'Accuracy/{phase}', acc, epoch)\n",
    "        \n",
    "        # Log learning rate\n",
    "        if phase == 'train':\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            self.writer.add_scalar('LR', lr, epoch)\n",
    "    \n",
    "    def _save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler': self.scheduler.state_dict(),\n",
    "            'best_val_acc': self.best_val_acc\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = os.path.join(self.exp_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "        torch.save(state, checkpoint_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            best_path = os.path.join(self.exp_dir, 'best_model.pth')\n",
    "            torch.save(state, best_path)\n",
    "    \n",
    "    def _save_attention_maps(self, attention_weights, epoch):\n",
    "        \"\"\"Save attention weights for visualization\"\"\"\n",
    "        attention_path = os.path.join(\n",
    "            self.attention_dir,\n",
    "            f'attention_epoch_{epoch}.npy'\n",
    "        )\n",
    "        np.save(attention_path, attention_weights.cpu().numpy())\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for inputs, labels in self.train_loader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            #print(labels)\n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, attention_weights = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Compute metrics\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_acc += self._compute_metrics(outputs, labels) * batch_size\n",
    "            total_samples += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_acc / total_samples\n",
    "        \n",
    "        return {\n",
    "            'loss': epoch_loss,\n",
    "            'accuracy': epoch_acc,\n",
    "            'attention_weights': attention_weights if self.config['save_attention_maps'] else None\n",
    "        }\n",
    "    \n",
    "    def validate_epoch(self, epoch):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        total_samples = 0\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "     \n",
    "                # Forward pass\n",
    "                outputs, attention_weights = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                # Compute metrics\n",
    "                batch_size = inputs.size(0)\n",
    "                running_loss += loss.item() * batch_size\n",
    "                running_acc += self._compute_metrics(outputs, labels) * batch_size\n",
    "                total_samples += batch_size\n",
    "                \n",
    "                if self.config['save_attention_maps']:\n",
    "                    all_attention_weights.append(attention_weights.cpu())\n",
    "        \n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_acc / total_samples\n",
    "        \n",
    "        attention_weights = torch.cat(all_attention_weights) if all_attention_weights else None\n",
    "        \n",
    "        return {\n",
    "            'loss': epoch_loss,\n",
    "            'accuracy': epoch_acc,\n",
    "            'attention_weights': attention_weights\n",
    "        }\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.config['epochs']):\n",
    "            # Train and validate\n",
    "            train_metrics = self.train_epoch(epoch)\n",
    "            val_metrics = self.validate_epoch(epoch)\n",
    "            \n",
    "            # Log metrics\n",
    "            self._log_metrics('train', train_metrics, epoch)\n",
    "            self._log_metrics('val', val_metrics, epoch)\n",
    "            \n",
    "            # Save attention maps periodically\n",
    "            if (self.config['save_attention_maps'] and \n",
    "                (epoch % self.config['attention_map_freq'] == 0 or epoch == self.config['epochs']-1)):\n",
    "                self._save_attention_maps(val_metrics['attention_weights'], epoch)\n",
    "            \n",
    "            # Step scheduler\n",
    "            self.scheduler.step(val_metrics['accuracy'])\n",
    "            \n",
    "            # Check for best model\n",
    "            if val_metrics['accuracy'] > self.best_val_acc:\n",
    "                self.best_val_acc = val_metrics['accuracy']\n",
    "                self._save_checkpoint(epoch, is_best=True)\n",
    "                self.early_stop_counter = 0\n",
    "            else:\n",
    "                self.early_stop_counter += 1\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if epoch % 10 == 0:\n",
    "                self._save_checkpoint(epoch)\n",
    "            \n",
    "            # Early stopping\n",
    "            # if self.early_stop_counter >= self.config['patience']:\n",
    "            #     print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            #     break\n",
    "        \n",
    "        # Training complete\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time//60:.0f}m {training_time%60:.0f}s\")\n",
    "        print(f\"Best validation accuracy: {self.best_val_acc:.2%}\")\n",
    "        \n",
    "        # Test if test loader provided\n",
    "        if self.test_loader:\n",
    "            test_acc = self.test()\n",
    "            print(f\"Test accuracy: {test_acc:.2%}\")\n",
    "        \n",
    "        # Close tensorboard writer\n",
    "        self.writer.close()\n",
    "        \n",
    "        return self.best_val_acc\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"Evaluate on test set\"\"\"\n",
    "        self.model.eval()\n",
    "        running_acc = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Load best model\n",
    "        best_path = os.path.join(self.exp_dir, 'best_model.pth')\n",
    "        if os.path.exists(best_path):\n",
    "            checkpoint = torch.load(best_path)\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                outputs, _ = self.model(inputs)\n",
    "                running_acc += self._compute_metrics(outputs, labels) * labels.size(0)\n",
    "                total_samples += labels.size(0)\n",
    "        \n",
    "        test_acc = running_acc / total_samples\n",
    "        self.writer.add_scalar('Accuracy/test', test_acc)\n",
    "        \n",
    "        return test_acc\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example configuration\n",
    "    config = {\n",
    "        'lr': 3e-4,\n",
    "        'weight_decay': 1e-5,\n",
    "        'patience': 15,\n",
    "        'epochs': 500,\n",
    "        'experiment_name': 'eeg_attention_experiment',\n",
    "        'save_attention_maps': True\n",
    "    }\n",
    "    \n",
    "    # Initialize components (using the previous data loader and model code)\n",
    "    model = EEGAttentionNet(num_channels=22, num_classes=4, timepoints=1125)\n",
    "    train_loader, val_loader, test_loader = get_data_loaders(data_dir=\"/teamspace/studios/this_studio/EEG_REC\", subjects = [1,2,3,5,6,7,8,9])\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = EEGTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    best_val_acc = trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
